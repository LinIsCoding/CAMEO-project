{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from spacy.symbols import ORTH\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file into DataFrame\n",
    "df = pd.read_csv('./CAMEO_IDEA_labeled_data.csv')\n",
    "\n",
    "# separate content and label\n",
    "text = df['Content']\n",
    "labels = df['Category Code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion tokenize sentence\n",
    "tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = tokenizer.Defaults.stop_words\n",
    "# tokenize, lemmatize the text, drop punctuations and stopwords\n",
    "tokenize = lambda t: [token.lemma_ for token in tokenizer(t) if (not token.is_punct) and (not token.is_stop)]\n",
    "\n",
    "# only tokenize the text\n",
    "# tokenize = lambda t: [token.text for token in tokenizer(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary <key=word : value=count>\n",
    "cnt = Counter()\n",
    "size = text.size\n",
    "for idx in range(size):\n",
    "    for word in tokenize(text[idx]):\n",
    "        cnt[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = dict(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diehard 1\n",
      "Croat 4\n",
      "fighter 4\n",
      "surrender 2\n",
      "serbian 14\n",
      "force 71\n",
      "Monday 62\n",
      "86-day 1\n",
      "siege 7\n",
      "bosnian 16\n"
     ]
    }
   ],
   "source": [
    "counter\n",
    "c = 0\n",
    "for key in counter:\n",
    "    if c < 10:\n",
    "        print (key, counter[key])\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out low-frequency word\n",
    "min_threshold = 1\n",
    "count = {x: count for x, count in cnt.items() if count >= min_threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out high-frequency word\n",
    "# max_threshold = 1\n",
    "# count = {x: count for x, count in cnt.items() if count <= max_threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diehard 1\n",
      "Croat 4\n",
      "fighter 4\n",
      "surrender 2\n",
      "serbian 14\n",
      "force 71\n",
      "Monday 62\n",
      "86-day 1\n",
      "siege 7\n",
      "bosnian 16\n"
     ]
    }
   ],
   "source": [
    "count\n",
    "c = 0\n",
    "for key in count:\n",
    "    if c < 10:\n",
    "        print (key, count[key])\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(text)\n",
    "y = np.array(labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download glove dictionary\n",
    "# def download_glove():\n",
    "#     ! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#     ! unzip glove.6B.zip -C data\n",
    "    \n",
    "# download_glove()\n",
    "# ! unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embedding dictionary (<key=word : value=vector>)\n",
    "def load_embedding_dict():\n",
    "    embeddings_dict = {}\n",
    "    with open(\"glove.6B.50d.txt\", 'r') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "glove_dic = load_embedding_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the :  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      ", :  [ 0.013441  0.23682  -0.16899   0.40951   0.63812   0.47709  -0.42852\n",
      " -0.55641  -0.364    -0.23938   0.13001  -0.063734 -0.39575  -0.48162\n",
      "  0.23291   0.090201 -0.13324   0.078639 -0.41634  -0.15428   0.10068\n",
      "  0.48891   0.31226  -0.1252   -0.037512 -1.5179    0.12612  -0.02442\n",
      " -0.042961 -0.28351   3.5416   -0.11956  -0.014533 -0.1499    0.21864\n",
      " -0.33412  -0.13872   0.31806   0.70358   0.44858  -0.080262  0.63003\n",
      "  0.32111  -0.46765   0.22786   0.36034  -0.37818  -0.56657   0.044691\n",
      "  0.30392 ]\n",
      ". :  [ 1.5164e-01  3.0177e-01 -1.6763e-01  1.7684e-01  3.1719e-01  3.3973e-01\n",
      " -4.3478e-01 -3.1086e-01 -4.4999e-01 -2.9486e-01  1.6608e-01  1.1963e-01\n",
      " -4.1328e-01 -4.2353e-01  5.9868e-01  2.8825e-01 -1.1547e-01 -4.1848e-02\n",
      " -6.7989e-01 -2.5063e-01  1.8472e-01  8.6876e-02  4.6582e-01  1.5035e-02\n",
      "  4.3474e-02 -1.4671e+00 -3.0384e-01 -2.3441e-02  3.0589e-01 -2.1785e-01\n",
      "  3.7460e+00  4.2284e-03 -1.8436e-01 -4.6209e-01  9.8329e-02 -1.1907e-01\n",
      "  2.3919e-01  1.1610e-01  4.1705e-01  5.6763e-02 -6.3681e-05  6.8987e-02\n",
      "  8.7939e-02 -1.0285e-01 -1.3931e-01  2.2314e-01 -8.0803e-02 -3.5652e-01\n",
      "  1.6413e-02  1.0216e-01]\n"
     ]
    }
   ],
   "source": [
    "glove_dic\n",
    "c=0\n",
    "for key in glove_dic:\n",
    "    if c < 3:\n",
    "        print (key,': ', glove_dic[key])\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries(<key=word : value=index number>) (<key=word : value=vector>)\n",
    "def create_embedding_matrix(count,emb_size=50):\n",
    "    size = len(count) + 2\n",
    "    word_idx_dict = {}\n",
    "    word_vec = np.zeros((size, emb_size), dtype=\"float32\")\n",
    "    \n",
    "    # add padding and UNK keyword\n",
    "    word_idx_dict[\"\"] = 0\n",
    "    word_vec[0] = np.zeros(emb_size, dtype='float32')\n",
    "    word_idx_dict[\"UNK\"] = 1\n",
    "    word_vec[1] = np.random.uniform(-0.25, 0.25, emb_size)\n",
    "\n",
    "    for i, word in enumerate(count.keys()):\n",
    "        word_idx_dict[word] = i + 2\n",
    "\n",
    "        if word in glove_dic:\n",
    "            word_vec[i + 2] = glove_dic[word]\n",
    "        else:\n",
    "            word_vec[i + 2] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "\n",
    "    return word_idx_dict, word_vec\n",
    "    \n",
    "word_idx_dict, pretrained_weight = create_embedding_matrix(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the :  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      ", :  [ 0.013441  0.23682  -0.16899   0.40951   0.63812   0.47709  -0.42852\n",
      " -0.55641  -0.364    -0.23938   0.13001  -0.063734 -0.39575  -0.48162\n",
      "  0.23291   0.090201 -0.13324   0.078639 -0.41634  -0.15428   0.10068\n",
      "  0.48891   0.31226  -0.1252   -0.037512 -1.5179    0.12612  -0.02442\n",
      " -0.042961 -0.28351   3.5416   -0.11956  -0.014533 -0.1499    0.21864\n",
      " -0.33412  -0.13872   0.31806   0.70358   0.44858  -0.080262  0.63003\n",
      "  0.32111  -0.46765   0.22786   0.36034  -0.37818  -0.56657   0.044691\n",
      "  0.30392 ]\n",
      ". :  [ 1.5164e-01  3.0177e-01 -1.6763e-01  1.7684e-01  3.1719e-01  3.3973e-01\n",
      " -4.3478e-01 -3.1086e-01 -4.4999e-01 -2.9486e-01  1.6608e-01  1.1963e-01\n",
      " -4.1328e-01 -4.2353e-01  5.9868e-01  2.8825e-01 -1.1547e-01 -4.1848e-02\n",
      " -6.7989e-01 -2.5063e-01  1.8472e-01  8.6876e-02  4.6582e-01  1.5035e-02\n",
      "  4.3474e-02 -1.4671e+00 -3.0384e-01 -2.3441e-02  3.0589e-01 -2.1785e-01\n",
      "  3.7460e+00  4.2284e-03 -1.8436e-01 -4.6209e-01  9.8329e-02 -1.1907e-01\n",
      "  2.3919e-01  1.1610e-01  4.1705e-01  5.6763e-02 -6.3681e-05  6.8987e-02\n",
      "  8.7939e-02 -1.0285e-01 -1.3931e-01  2.2314e-01 -8.0803e-02 -3.5652e-01\n",
      "  1.6413e-02  1.0216e-01]\n",
      "of :  [ 0.70853    0.57088   -0.4716     0.18048    0.54449    0.72603\n",
      "  0.18157   -0.52393    0.10381   -0.17566    0.078852  -0.36216\n",
      " -0.11829   -0.83336    0.11917   -0.16605    0.061555  -0.012719\n",
      " -0.56623    0.013616   0.22851   -0.14396   -0.067549  -0.38157\n",
      " -0.23698   -1.7037    -0.86692   -0.26704   -0.2589     0.1767\n",
      "  3.8676    -0.1613    -0.13273   -0.68881    0.18444    0.0052464\n",
      " -0.33874   -0.078956   0.24185    0.36576   -0.34727    0.28483\n",
      "  0.075693  -0.062178  -0.38988    0.22902   -0.21617   -0.22562\n",
      " -0.093918  -0.80375  ]\n",
      "to :  [ 0.68047  -0.039263  0.30186  -0.17792   0.42962   0.032246 -0.41376\n",
      "  0.13228  -0.29847  -0.085253  0.17118   0.22419  -0.10046  -0.43653\n",
      "  0.33418   0.67846   0.057204 -0.34448  -0.42785  -0.43275   0.55963\n",
      "  0.10032   0.18677  -0.26854   0.037334 -2.0932    0.22171  -0.39868\n",
      "  0.20912  -0.55725   3.8826    0.47466  -0.95658  -0.37788   0.20869\n",
      " -0.32752   0.12751   0.088359  0.16351  -0.21634  -0.094375  0.018324\n",
      "  0.21048  -0.03088  -0.19722   0.082279 -0.09434  -0.073297 -0.064699\n",
      " -0.26044 ]\n"
     ]
    }
   ],
   "source": [
    "word_idx_dict\n",
    "c=0\n",
    "for key in glove_dic:\n",
    "    if c < 5:\n",
    "        print (key,': ', glove_dic[key])\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.07017878, -0.04507172, -0.14554831, -0.09171732,  0.13337933,\n",
       "         0.02589959,  0.2437299 ,  0.06947749,  0.14554648, -0.01943418,\n",
       "         0.22567232,  0.03857403,  0.05386337,  0.24910834, -0.16631441,\n",
       "         0.03747147,  0.23714148,  0.09276155,  0.02601584, -0.12251747,\n",
       "         0.06519696,  0.16126014,  0.18456398, -0.16561438,  0.21618271,\n",
       "         0.14147273, -0.22169572, -0.21732807, -0.07712799,  0.14286834,\n",
       "         0.22942741,  0.07251368, -0.16869912, -0.20313576,  0.17457905,\n",
       "         0.14385584, -0.05228975, -0.00138679,  0.10377755,  0.0905782 ,\n",
       "         0.13586673,  0.12265873, -0.07283758, -0.08971459, -0.23289633,\n",
       "        -0.21581441,  0.21991177, -0.06718753, -0.04704488,  0.16500251],\n",
       "       [ 0.13406347,  0.24224102,  0.19126883,  0.1633965 ,  0.24819592,\n",
       "        -0.20472604,  0.18375455,  0.08320342, -0.17300141,  0.17735104,\n",
       "        -0.15839566,  0.0482519 ,  0.07702441,  0.15777327, -0.01623654,\n",
       "        -0.1289578 ,  0.1395048 , -0.09229822,  0.11490471,  0.04635965,\n",
       "         0.19421634, -0.17841068,  0.13718045,  0.13574274,  0.22482412,\n",
       "         0.2307793 , -0.10358252,  0.24039046,  0.0550249 , -0.14442515,\n",
       "        -0.02692852, -0.10606039, -0.00095543,  0.03581464,  0.12807871,\n",
       "         0.17959696, -0.02454757,  0.09728455, -0.0051447 ,  0.2010897 ,\n",
       "        -0.01616802, -0.11394362,  0.24136305,  0.2423048 ,  0.08890688,\n",
       "        -0.186251  , -0.05437649, -0.17944507,  0.05878004,  0.06972833]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_weight[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for encoding sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(line, word_idx_dict, N=400, padding_start=True):\n",
    "    tokens = tokenize(line)\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([word_idx_dict.get(word, word_idx_dict[\"UNK\"]) for word in tokens])\n",
    "    length = min(N, len(enc1))\n",
    "    if padding_start:\n",
    "        enc[:length] = enc1[:length]\n",
    "    else:\n",
    "        enc[N - length:] = enc1[:length]\n",
    "    return enc, length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DataSet and DataLoader for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventDataset(Dataset):\n",
    "    def __init__(self, X, y, N=40, padding_start=False):\n",
    "        self.y = y\n",
    "        self.X = [encode_sentence(line, word_idx_dict, N, padding_start) for line in X]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, s = self.X[idx]\n",
    "        return x, s, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Kuwaiti resistance fighters staged a suicide bomb attack on Iraqi targets, Kuwait's ambassador to France said on Friday.\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0, 651, 652,   4, 653,\n",
       "        654, 641, 655, 484, 656, 657, 205, 658,  57, 194], dtype=int32), 14)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence(X_train[0],word_idx_dict,400,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = EventDataset(X_train, y_train)\n",
    "valid_ds = EventDataset(X_val, y_val)\n",
    "train_dl = DataLoader(train_ds, batch_size=30, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,  147,  113, 1080, 1081, 1082, 1083,  222,\n",
       "         259,   69,  335,  193, 1084, 1085, 1086], dtype=int32), 14, 0)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[   0,    0,    0,  ..., 1745, 1746, 1747],\n",
       "         [   0,    0,    0,  ..., 1084, 1085, 1086],\n",
       "         [   0,    0,    0,  ...,  646,   57,  349],\n",
       "         ...,\n",
       "         [   0,    0,    0,  ..., 1063, 1935, 1609],\n",
       "         [   0,    0,    0,  ..., 1099,  259,   88],\n",
       "         [   0,    0,    0,  ...,  110,   57,   88]], dtype=torch.int32),\n",
       " tensor([16, 14, 22, 15, 24, 16, 10, 12,  7, 15, 13,  6, 11, 14, 15, 15, 11,  5,\n",
       "         16, 16, 20, 18, 13, 19, 12, 16,  7, 15, 20, 15]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(valid_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_optimizer(optimizer, lr):\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, optimizer, train_dl, val_dl, epochs=10):\n",
    "    global max_acc\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, s, y in train_dl:\n",
    "            # s is not used in this model\n",
    "            x = x.long() \n",
    "            y = y.long() \n",
    "            y_pred = model(x)           \n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc = val_metrics(model, val_dl)\n",
    "        if val_acc > max_acc:\n",
    "            max_acc = val_acc\n",
    "#         if i % 5 == 0:\n",
    "        print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "f1 = False\n",
    "def val_metrics(model, val_dl):\n",
    "    global f1\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    for x, s, y in val_dl:\n",
    "        x = x.long()  #.cuda()\n",
    "        y = y.long()\n",
    "        batch = y.shape[0]\n",
    "#         print(y.size())\n",
    "        out = model(x)\n",
    "        \n",
    "        loss = F.cross_entropy(out, y)\n",
    "        sum_loss += batch*(loss.item())\n",
    "        total += batch\n",
    "        _, pred = torch.max(out, 1) \n",
    "#         print(pred.numpy())\n",
    "#         print(y.data.numpy())\n",
    "        if f1 == False:\n",
    "            f1_weighted = f1_score(y.data.numpy(), pred.numpy(), average='weighted')\n",
    "            print('f1', f1_weighted)\n",
    "            f1 = True\n",
    "        correct += (pred == y.data).float().sum().item()\n",
    "    val_loss = sum_loss/total\n",
    "    val_acc = correct/total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights=None) :\n",
    "        super(GRUModel,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if glove_weights is not None:\n",
    "            self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "            self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "            \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        out_pack, ht = self.gru(x)\n",
    "        x = self.linear(ht[-1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-1.5964, -1.3766,  1.4420,  ..., -1.7100, -1.3228,  2.1490],\n",
       "         [-0.1428, -1.4456,  0.1120,  ..., -0.3541, -1.7548, -0.3913],\n",
       "         [ 2.2461, -2.0732, -0.7869,  ...,  0.2735, -2.7883, -0.0315]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.5526, -0.0541, -0.2870,  ...,  1.3877,  1.8290, -0.1891],\n",
       "         [-0.3558, -1.4996, -1.7340,  ...,  0.4983, -1.2700,  0.3907],\n",
       "         [-0.7816, -1.0165,  1.7426,  ..., -0.4021,  0.3671, -1.8707]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.3462,  0.0927,  0.4001,  ...,  0.4297, -0.8334,  0.1958],\n",
       "         [-0.0895, -1.4933,  1.1138,  ...,  0.3957,  0.8764,  0.1097],\n",
       "         [-0.3140,  0.5531, -1.6016,  ..., -2.3476, -0.2918, -0.5649]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.4462, -0.6720,  0.6773,  ...,  0.5662,  1.8643, -0.7261],\n",
       "         [ 1.0185, -0.3469,  2.1344,  ..., -0.5138, -0.3126, -1.1497],\n",
       "         [ 1.2155, -0.5805, -0.5988,  ...,  0.4326,  1.2534, -1.0179]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.4513,  0.3284, -1.3804,  ...,  0.2618, -0.2305, -0.0102],\n",
       "         [ 0.8318, -0.2174,  0.5856,  ...,  0.1468, -1.9541, -1.0811],\n",
       "         [ 0.0069,  0.4913, -0.0236,  ...,  1.1781, -1.1793,  0.4220]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-0.9425, -0.6032, -0.1501,  ...,  0.8679,  0.3922, -0.2571],\n",
       "         [-0.0895, -1.4933,  1.1138,  ...,  0.3957,  0.8764,  0.1097],\n",
       "         [ 0.0069,  0.4913, -0.0236,  ...,  1.1781, -1.1793,  0.4220]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, s, y = next(iter(valid_dl))\n",
    "embeddings = nn.Embedding(vocab_size, 50, padding_idx=0)\n",
    "embeddings(x.long())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "         True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, s, y = next(iter(valid_dl))\n",
    "out = model(x.long())\n",
    "_, pred = torch.max(out, 1) \n",
    "pred\n",
    "pred == y.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.908395061728395\n",
      "train loss 0.643 val loss 0.543 and val accuracy 0.809\n",
      "train loss 0.444 val loss 0.526 and val accuracy 0.829\n",
      "train loss 0.432 val loss 0.599 and val accuracy 0.819\n",
      "train loss 0.406 val loss 0.549 and val accuracy 0.824\n",
      "train loss 0.396 val loss 0.543 and val accuracy 0.814\n",
      "train loss 0.351 val loss 0.517 and val accuracy 0.829\n",
      "train loss 0.348 val loss 0.526 and val accuracy 0.834\n",
      "train loss 0.331 val loss 0.577 and val accuracy 0.824\n",
      "train loss 0.283 val loss 0.571 and val accuracy 0.824\n",
      "train loss 0.269 val loss 0.530 and val accuracy 0.809\n",
      "train loss 0.272 val loss 0.644 and val accuracy 0.794\n",
      "train loss 0.248 val loss 0.539 and val accuracy 0.819\n",
      "train loss 0.266 val loss 0.668 and val accuracy 0.824\n",
      "train loss 0.224 val loss 0.613 and val accuracy 0.809\n",
      "train loss 0.221 val loss 0.769 and val accuracy 0.829\n",
      "train loss 0.204 val loss 0.714 and val accuracy 0.799\n",
      "train loss 0.207 val loss 0.691 and val accuracy 0.814\n",
      "train loss 0.185 val loss 0.983 and val accuracy 0.804\n",
      "train loss 0.222 val loss 0.646 and val accuracy 0.829\n",
      "train loss 0.177 val loss 0.795 and val accuracy 0.819\n",
      "train loss 0.187 val loss 0.712 and val accuracy 0.794\n",
      "train loss 0.197 val loss 0.839 and val accuracy 0.799\n",
      "train loss 0.177 val loss 0.654 and val accuracy 0.794\n",
      "train loss 0.165 val loss 0.752 and val accuracy 0.814\n",
      "train loss 0.167 val loss 0.733 and val accuracy 0.809\n",
      "train loss 0.175 val loss 0.796 and val accuracy 0.814\n",
      "train loss 0.217 val loss 0.759 and val accuracy 0.759\n",
      "train loss 0.210 val loss 0.767 and val accuracy 0.789\n",
      "train loss 0.225 val loss 0.662 and val accuracy 0.799\n",
      "train loss 0.284 val loss 0.737 and val accuracy 0.819\n",
      "train loss 0.241 val loss 0.759 and val accuracy 0.794\n",
      "train loss 0.248 val loss 0.842 and val accuracy 0.794\n",
      "train loss 0.242 val loss 0.796 and val accuracy 0.799\n",
      "train loss 0.296 val loss 0.784 and val accuracy 0.779\n",
      "train loss 0.256 val loss 0.876 and val accuracy 0.824\n",
      "train loss 0.325 val loss 0.722 and val accuracy 0.779\n",
      "train loss 0.290 val loss 0.772 and val accuracy 0.804\n",
      "train loss 0.296 val loss 0.711 and val accuracy 0.824\n",
      "train loss 0.319 val loss 0.643 and val accuracy 0.814\n",
      "train loss 0.281 val loss 0.707 and val accuracy 0.814\n",
      "train loss 0.270 val loss 0.784 and val accuracy 0.794\n",
      "train loss 0.250 val loss 0.850 and val accuracy 0.804\n",
      "train loss 0.298 val loss 0.814 and val accuracy 0.789\n",
      "train loss 0.370 val loss 0.856 and val accuracy 0.779\n",
      "train loss 0.273 val loss 0.813 and val accuracy 0.804\n",
      "train loss 0.280 val loss 0.733 and val accuracy 0.814\n",
      "train loss 0.248 val loss 0.708 and val accuracy 0.809\n",
      "train loss 0.280 val loss 0.631 and val accuracy 0.774\n",
      "train loss 0.260 val loss 0.639 and val accuracy 0.814\n",
      "train loss 0.284 val loss 0.572 and val accuracy 0.784\n",
      "train loss 0.298 val loss 0.617 and val accuracy 0.839\n",
      "train loss 0.287 val loss 0.693 and val accuracy 0.809\n",
      "train loss 0.290 val loss 0.731 and val accuracy 0.819\n",
      "train loss 0.318 val loss 0.792 and val accuracy 0.769\n",
      "train loss 0.312 val loss 0.810 and val accuracy 0.769\n",
      "train loss 0.483 val loss 0.867 and val accuracy 0.779\n",
      "train loss 0.480 val loss 0.764 and val accuracy 0.764\n",
      "train loss 0.448 val loss 0.675 and val accuracy 0.799\n",
      "train loss 0.473 val loss 0.733 and val accuracy 0.759\n",
      "train loss 0.526 val loss 0.598 and val accuracy 0.784\n",
      "0.8391959798994975\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_idx_dict)\n",
    "model = GRUModel(vocab_size, 50, 50*(i+1), glove_weights = pretrained_weight)\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.01)\n",
    "max_acc = 0\n",
    "train_epocs(model, optimizer, train_dl, valid_dl, epochs=60)\n",
    "print(max_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.7493333333333332\n",
      "train loss 0.653 val loss 0.470 and val accuracy 0.804\n",
      "train loss 0.440 val loss 0.435 and val accuracy 0.819\n",
      "train loss 0.348 val loss 0.511 and val accuracy 0.829\n",
      "train loss 0.347 val loss 0.497 and val accuracy 0.814\n",
      "train loss 0.281 val loss 0.551 and val accuracy 0.799\n",
      "train loss 0.251 val loss 0.538 and val accuracy 0.824\n",
      "train loss 0.257 val loss 0.531 and val accuracy 0.819\n",
      "train loss 0.246 val loss 0.593 and val accuracy 0.819\n",
      "train loss 0.234 val loss 0.632 and val accuracy 0.824\n",
      "train loss 0.255 val loss 0.623 and val accuracy 0.799\n",
      "train loss 0.222 val loss 0.602 and val accuracy 0.799\n",
      "train loss 0.211 val loss 0.572 and val accuracy 0.804\n",
      "0.8391959798994975\n",
      "train loss 0.870 val loss 0.553 and val accuracy 0.739\n",
      "train loss 0.893 val loss 0.805 and val accuracy 0.729\n",
      "train loss 0.771 val loss 0.720 and val accuracy 0.744\n",
      "train loss 0.705 val loss 0.735 and val accuracy 0.774\n",
      "train loss 0.693 val loss 0.727 and val accuracy 0.734\n",
      "train loss 0.661 val loss 0.593 and val accuracy 0.759\n",
      "train loss 0.681 val loss 0.783 and val accuracy 0.714\n",
      "train loss 0.685 val loss 0.630 and val accuracy 0.789\n",
      "train loss 0.771 val loss 0.766 and val accuracy 0.698\n",
      "train loss 0.653 val loss 0.560 and val accuracy 0.774\n",
      "train loss 0.577 val loss 0.620 and val accuracy 0.759\n",
      "train loss 0.675 val loss 0.668 and val accuracy 0.744\n",
      "0.7989949748743719\n",
      "train loss 0.649 val loss 0.498 and val accuracy 0.769\n",
      "train loss 0.409 val loss 0.500 and val accuracy 0.814\n",
      "train loss 0.349 val loss 0.512 and val accuracy 0.834\n",
      "train loss 0.288 val loss 0.510 and val accuracy 0.799\n",
      "train loss 0.243 val loss 0.534 and val accuracy 0.794\n",
      "train loss 0.218 val loss 0.547 and val accuracy 0.829\n",
      "train loss 0.187 val loss 0.673 and val accuracy 0.829\n",
      "train loss 0.249 val loss 0.594 and val accuracy 0.819\n",
      "train loss 0.226 val loss 0.700 and val accuracy 0.809\n",
      "train loss 0.251 val loss 0.675 and val accuracy 0.799\n",
      "train loss 0.228 val loss 0.597 and val accuracy 0.809\n",
      "train loss 0.246 val loss 0.617 and val accuracy 0.834\n",
      "0.864321608040201\n",
      "train loss 1.419 val loss 0.944 and val accuracy 0.749\n",
      "train loss 0.634 val loss 0.504 and val accuracy 0.804\n",
      "train loss 0.740 val loss 0.868 and val accuracy 0.769\n",
      "train loss 0.530 val loss 0.762 and val accuracy 0.769\n",
      "train loss 0.562 val loss 0.768 and val accuracy 0.744\n",
      "train loss 0.603 val loss 0.569 and val accuracy 0.809\n",
      "train loss 0.749 val loss 0.956 and val accuracy 0.774\n",
      "train loss 0.772 val loss 0.632 and val accuracy 0.794\n",
      "train loss 1.082 val loss 1.156 and val accuracy 0.658\n",
      "train loss 0.674 val loss 1.279 and val accuracy 0.206\n",
      "train loss 0.915 val loss 0.658 and val accuracy 0.829\n",
      "train loss 0.677 val loss 0.814 and val accuracy 0.754\n",
      "0.8391959798994975\n",
      "train loss 0.676 val loss 0.567 and val accuracy 0.754\n",
      "train loss 0.415 val loss 0.450 and val accuracy 0.824\n",
      "train loss 0.329 val loss 0.553 and val accuracy 0.829\n",
      "train loss 0.255 val loss 0.539 and val accuracy 0.814\n",
      "train loss 0.250 val loss 0.570 and val accuracy 0.764\n",
      "train loss 0.273 val loss 0.721 and val accuracy 0.809\n",
      "train loss 0.277 val loss 0.567 and val accuracy 0.824\n",
      "train loss 0.300 val loss 0.623 and val accuracy 0.774\n",
      "train loss 0.317 val loss 0.529 and val accuracy 0.819\n",
      "train loss 0.284 val loss 0.606 and val accuracy 0.799\n",
      "train loss 0.526 val loss 0.639 and val accuracy 0.789\n",
      "train loss 0.491 val loss 0.596 and val accuracy 0.814\n",
      "0.8492462311557789\n",
      "train loss 1.633 val loss 1.161 and val accuracy 0.764\n",
      "train loss 0.954 val loss 0.662 and val accuracy 0.774\n",
      "train loss 1.425 val loss 1.697 and val accuracy 0.739\n",
      "train loss 0.734 val loss 0.638 and val accuracy 0.744\n",
      "train loss 1.237 val loss 0.736 and val accuracy 0.764\n",
      "train loss 0.923 val loss 0.954 and val accuracy 0.779\n",
      "train loss 0.783 val loss 1.277 and val accuracy 0.769\n",
      "train loss 1.047 val loss 0.927 and val accuracy 0.744\n",
      "train loss 0.754 val loss 0.728 and val accuracy 0.764\n",
      "train loss 0.872 val loss 1.168 and val accuracy 0.759\n",
      "train loss 0.961 val loss 0.767 and val accuracy 0.754\n",
      "train loss 1.069 val loss 0.897 and val accuracy 0.764\n",
      "0.7889447236180904\n",
      "train loss 0.738 val loss 0.568 and val accuracy 0.769\n",
      "train loss 0.433 val loss 0.474 and val accuracy 0.804\n",
      "train loss 0.348 val loss 0.585 and val accuracy 0.799\n",
      "train loss 0.272 val loss 0.543 and val accuracy 0.809\n",
      "train loss 0.248 val loss 0.666 and val accuracy 0.784\n",
      "train loss 0.235 val loss 0.609 and val accuracy 0.809\n",
      "train loss 0.228 val loss 0.634 and val accuracy 0.809\n",
      "train loss 0.250 val loss 0.615 and val accuracy 0.804\n",
      "train loss 0.284 val loss 0.642 and val accuracy 0.779\n",
      "train loss 0.262 val loss 0.627 and val accuracy 0.809\n",
      "train loss 0.259 val loss 0.653 and val accuracy 0.819\n",
      "train loss 0.267 val loss 0.742 and val accuracy 0.794\n",
      "0.8341708542713567\n",
      "train loss 2.085 val loss 1.905 and val accuracy 0.754\n",
      "train loss 0.843 val loss 0.931 and val accuracy 0.769\n",
      "train loss 0.938 val loss 0.705 and val accuracy 0.784\n",
      "train loss 0.997 val loss 1.417 and val accuracy 0.774\n",
      "train loss 1.002 val loss 1.059 and val accuracy 0.724\n",
      "train loss 0.989 val loss 1.111 and val accuracy 0.784\n",
      "train loss 1.216 val loss 1.582 and val accuracy 0.417\n",
      "train loss 1.119 val loss 1.105 and val accuracy 0.789\n",
      "train loss 1.187 val loss 1.194 and val accuracy 0.693\n",
      "train loss 1.144 val loss 1.492 and val accuracy 0.754\n",
      "train loss 1.258 val loss 1.988 and val accuracy 0.744\n",
      "train loss 1.168 val loss 1.024 and val accuracy 0.779\n",
      "0.7939698492462312\n"
     ]
    }
   ],
   "source": [
    "for i in range(4): \n",
    "    vocab_size = len(word_idx_dict)\n",
    "    model = GRUModel(vocab_size, 50, 50*(i+1), glove_weights = pretrained_weight)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.01)\n",
    "    max_acc = 0\n",
    "    train_epocs(model, optimizer, train_dl, valid_dl, epochs=60)\n",
    "    print(max_acc)\n",
    "    \n",
    "    vocab_size = len(word_idx_dict)\n",
    "    model = GRUModel(vocab_size, 50, 50*(i+1), glove_weights = pretrained_weight)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.1)\n",
    "    max_acc = 0\n",
    "    train_epocs(model, optimizer, train_dl, valid_dl, epochs=60)\n",
    "    print(max_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decreasing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 2.593 val loss 1.238 and val accuracy 0.719\n",
      "train loss 0.598 val loss 0.486 and val accuracy 0.814\n",
      "train loss 0.608 val loss 0.690 and val accuracy 0.724\n",
      "train loss 0.653 val loss 0.570 and val accuracy 0.769\n",
      "train loss 0.832 val loss 0.779 and val accuracy 0.729\n",
      "train loss 0.807 val loss 0.647 and val accuracy 0.754\n",
      "train loss 0.694 val loss 0.774 and val accuracy 0.744\n",
      "train loss 0.547 val loss 0.667 and val accuracy 0.739\n",
      "train loss 0.520 val loss 0.638 and val accuracy 0.754\n",
      "train loss 0.502 val loss 0.668 and val accuracy 0.714\n",
      "train loss 0.523 val loss 0.849 and val accuracy 0.648\n",
      "train loss 0.516 val loss 0.762 and val accuracy 0.744\n"
     ]
    }
   ],
   "source": [
    "model = GRUModel1(vocab_size, 50, 50, 50, glove_weights = pretrained_weight)\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.1)\n",
    "max_acc = 0\n",
    "train_epocs(model, optimizer, train_dl, valid_dl, epochs=30)\n",
    "\n",
    "update_optimizer(optimizer, 0.005)\n",
    "train_epocs(model, optimizer, train_dl, valid_dl, epochs=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8241206030150754"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with two linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel1(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, hidden_dim1, glove_weights=None) :\n",
    "        super(GRUModel1,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if glove_weights is not None:\n",
    "            self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "            self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "            \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim1)\n",
    "        self.linear1 = nn.Linear(hidden_dim1, 3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        out_pack, ht = self.gru(x)\n",
    "        x = self.linear(ht[-1])\n",
    "        x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.704 val loss 0.526 and val accuracy 0.804\n",
      "train loss 0.447 val loss 0.468 and val accuracy 0.814\n",
      "train loss 0.376 val loss 0.480 and val accuracy 0.834\n",
      "train loss 0.326 val loss 0.612 and val accuracy 0.819\n",
      "train loss 0.309 val loss 0.519 and val accuracy 0.794\n",
      "train loss 0.284 val loss 0.502 and val accuracy 0.839\n",
      "0.8391959798994975\n",
      "train loss 2.369 val loss 0.861 and val accuracy 0.744\n",
      "train loss 0.680 val loss 0.678 and val accuracy 0.714\n",
      "train loss 0.980 val loss 1.079 and val accuracy 0.653\n",
      "train loss 3.440 val loss 1.982 and val accuracy 0.739\n",
      "train loss 0.842 val loss 0.928 and val accuracy 0.749\n",
      "train loss 0.767 val loss 0.737 and val accuracy 0.754\n",
      "0.7587939698492462\n",
      "train loss 0.738 val loss 0.568 and val accuracy 0.754\n",
      "train loss 0.437 val loss 0.504 and val accuracy 0.794\n",
      "train loss 0.360 val loss 0.509 and val accuracy 0.814\n",
      "train loss 0.306 val loss 0.616 and val accuracy 0.779\n",
      "train loss 0.271 val loss 0.666 and val accuracy 0.774\n",
      "train loss 0.225 val loss 0.816 and val accuracy 0.789\n",
      "0.8341708542713567\n",
      "train loss 4.190 val loss 2.998 and val accuracy 0.744\n",
      "train loss 0.850 val loss 1.118 and val accuracy 0.749\n",
      "train loss 1.664 val loss 1.509 and val accuracy 0.201\n",
      "train loss 0.712 val loss 0.953 and val accuracy 0.693\n",
      "train loss 0.668 val loss 0.702 and val accuracy 0.724\n",
      "train loss 0.687 val loss 0.736 and val accuracy 0.714\n",
      "0.7537688442211056\n",
      "train loss 0.757 val loss 0.606 and val accuracy 0.754\n",
      "train loss 0.449 val loss 0.495 and val accuracy 0.804\n",
      "train loss 0.381 val loss 0.494 and val accuracy 0.794\n",
      "train loss 0.354 val loss 0.562 and val accuracy 0.774\n",
      "train loss 0.230 val loss 0.656 and val accuracy 0.789\n",
      "train loss 0.240 val loss 0.537 and val accuracy 0.794\n",
      "0.8341708542713567\n",
      "train loss 8.876 val loss 1.209 and val accuracy 0.693\n",
      "train loss 0.712 val loss 0.759 and val accuracy 0.744\n",
      "train loss 0.676 val loss 0.650 and val accuracy 0.754\n",
      "train loss 0.651 val loss 0.692 and val accuracy 0.754\n",
      "train loss 5.736 val loss 2.706 and val accuracy 0.668\n",
      "train loss 0.875 val loss 1.085 and val accuracy 0.588\n",
      "0.7688442211055276\n",
      "train loss 0.831 val loss 0.675 and val accuracy 0.719\n",
      "train loss 0.456 val loss 0.488 and val accuracy 0.824\n",
      "train loss 0.351 val loss 0.501 and val accuracy 0.789\n",
      "train loss 0.321 val loss 0.519 and val accuracy 0.819\n",
      "train loss 0.301 val loss 0.529 and val accuracy 0.829\n",
      "train loss 0.270 val loss 0.617 and val accuracy 0.799\n",
      "0.8341708542713567\n",
      "train loss 10.990 val loss 3.498 and val accuracy 0.176\n",
      "train loss 0.770 val loss 0.683 and val accuracy 0.764\n",
      "train loss 0.645 val loss 0.666 and val accuracy 0.779\n",
      "train loss 0.600 val loss 0.592 and val accuracy 0.799\n",
      "train loss 28.754 val loss 74.356 and val accuracy 0.749\n",
      "train loss 1.730 val loss 1.209 and val accuracy 0.734\n",
      "0.8190954773869347\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    model1 = GRUModel1(vocab_size, 50, 50*(i+1), 50, glove_weights = pretrained_weight)\n",
    "    parameters1 = filter(lambda p: p.requires_grad, model1.parameters())\n",
    "    optimizer1 = torch.optim.Adam(parameters1, lr=0.01)\n",
    "    max_acc = 0\n",
    "    train_epocs(model1, optimizer1, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)\n",
    "    \n",
    "    model1 = GRUModel1(vocab_size, 50, 50*(i+1), 50, glove_weights = pretrained_weight)\n",
    "    parameters1 = filter(lambda p: p.requires_grad, model1.parameters())\n",
    "    optimizer1 = torch.optim.Adam(parameters1, lr=0.1)\n",
    "    max_acc = 0\n",
    "    train_epocs(model1, optimizer1, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
