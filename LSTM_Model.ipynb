{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from spacy.symbols import ORTH\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file into DataFrame\n",
    "df = pd.read_csv('./CAMEO_IDEA_labeled_data.csv')\n",
    "\n",
    "# separate content and label\n",
    "text = df['Content']\n",
    "labels = df['Category Code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion tokenize sentence\n",
    "tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = tokenizer.Defaults.stop_words\n",
    "# tokenize, lemmatize the text, drop punctuations and stopwords\n",
    "tokenize = lambda t: [token.lemma_ for token in tokenizer(t) if (not token.is_punct) and (not token.is_stop)]\n",
    "\n",
    "# only tokenize the text\n",
    "# tokenize = lambda t: [token.text for token in tokenizer(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary <key=word : value=count>\n",
    "cnt = Counter()\n",
    "size = text.size\n",
    "for idx in range(size):\n",
    "    for word in tokenize(text[idx]):\n",
    "        cnt[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out low-frequency word\n",
    "min_threshold = 1\n",
    "count = {x: count for x, count in cnt.items() if count >= min_threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out high-frequency word\n",
    "min_threshold = 1\n",
    "count = {x: count for x, count in cnt.items() if count <= min_threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(text)\n",
    "y = np.array(labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download glove dictionary\n",
    "# def download_glove():\n",
    "#     ! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#     ! unzip glove.6B.zip -C data\n",
    "    \n",
    "# download_glove()\n",
    "# ! unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embedding dictionary (<key=word : value=vector>)\n",
    "def load_embedding_dict():\n",
    "    embeddings_dict = {}\n",
    "    with open(\"glove.6B.50d.txt\", 'r') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "glove_dic = load_embedding_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries(<key=word : value=index number>) (<key=word : value=vector>)\n",
    "def create_embedding_matrix(count,emb_size=50):\n",
    "    size = len(count) + 2\n",
    "    word_idx_dict = {}\n",
    "    word_vec = np.zeros((size, emb_size), dtype=\"float32\")\n",
    "    \n",
    "    # add padding and UNK keyword\n",
    "    word_idx_dict[\"\"] = 0\n",
    "    word_vec[0] = np.zeros(emb_size, dtype='float32')\n",
    "    word_idx_dict[\"UNK\"] = 1\n",
    "    word_vec[1] = np.random.uniform(-0.25, 0.25, emb_size)\n",
    "\n",
    "    for i, word in enumerate(count.keys()):\n",
    "        word_idx_dict[word] = i + 2\n",
    "\n",
    "        if word in glove_dic:\n",
    "            word_vec[i + 2] = glove_dic[word]\n",
    "        else:\n",
    "            word_vec[i + 2] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "\n",
    "    return word_idx_dict, word_vec\n",
    "    \n",
    "word_idx_dict, pretrained_weight = create_embedding_matrix(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for encoding sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(line, word_idx_dict, N=400, padding_start=True):\n",
    "    tokens = tokenize(line)\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([word_idx_dict.get(word, word_idx_dict[\"UNK\"]) for word in tokens])\n",
    "    length = min(N, len(enc1))\n",
    "    if padding_start:\n",
    "        enc[:length] = enc1[:length]\n",
    "    else:\n",
    "        enc[N - length:] = enc1[:length]\n",
    "    return enc, length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DataSet and DataLoader for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventDataset(Dataset):\n",
    "    def __init__(self, X, y, N=40, padding_start=False):\n",
    "        self.y = y\n",
    "        self.X = [encode_sentence(line, word_idx_dict, N, padding_start) for line in X]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, s = self.X[idx]\n",
    "        return x, s, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = EventDataset(X_train, y_train)\n",
    "valid_ds = EventDataset(X_val, y_val)\n",
    "train_dl = DataLoader(train_ds, batch_size=30, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_optimizer(optimizer, lr):\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, optimizer, train_dl, val_dl, epochs=10):\n",
    "    global max_acc\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, s, y in train_dl:\n",
    "            x = x.long() \n",
    "            y = y.long() \n",
    "            y_pred = model(x)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred.squeeze(0), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc = val_metrics(model, val_dl)\n",
    "        if val_acc > max_acc:\n",
    "            max_acc = val_acc\n",
    "        if i % 5 == 1:\n",
    "            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, val_dl):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    for x, s, y in val_dl:\n",
    "        x = x.long()  \n",
    "        y = y.long()\n",
    "        batch = y.shape[0]\n",
    "        out = model(x)\n",
    "        loss = F.cross_entropy(out.squeeze(0), y)\n",
    "        sum_loss += batch*(loss.item())\n",
    "        total += batch\n",
    "        _, pred = torch.max(out.squeeze(0), 1) \n",
    "        correct += (pred == y.data).float().sum().item()\n",
    "    val_loss = sum_loss/total\n",
    "    val_acc = correct/total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights=None) :\n",
    "        super(LSTMModel,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if glove_weights is not None:\n",
    "            self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "            self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        out_pack, ht = self.lstm(x)\n",
    "        x = self.linear(ht[-1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.521 val loss 0.483 and val accuracy 0.819\n",
      "train loss 0.411 val loss 0.457 and val accuracy 0.814\n",
      "train loss 0.356 val loss 0.441 and val accuracy 0.854\n",
      "train loss 0.317 val loss 0.493 and val accuracy 0.834\n",
      "train loss 0.235 val loss 0.584 and val accuracy 0.824\n",
      "train loss 0.243 val loss 0.579 and val accuracy 0.809\n",
      "0.8542713567839196\n",
      "train loss 0.694 val loss 0.556 and val accuracy 0.769\n",
      "train loss 0.797 val loss 1.016 and val accuracy 0.784\n",
      "train loss 0.513 val loss 0.503 and val accuracy 0.809\n",
      "train loss 0.511 val loss 0.511 and val accuracy 0.759\n",
      "train loss 0.483 val loss 0.507 and val accuracy 0.804\n",
      "train loss 0.485 val loss 0.549 and val accuracy 0.779\n",
      "0.8291457286432161\n",
      "train loss 0.543 val loss 0.506 and val accuracy 0.784\n",
      "train loss 0.387 val loss 0.520 and val accuracy 0.849\n",
      "train loss 0.342 val loss 0.501 and val accuracy 0.834\n",
      "train loss 0.311 val loss 0.518 and val accuracy 0.789\n",
      "train loss 0.291 val loss 0.590 and val accuracy 0.794\n",
      "train loss 0.216 val loss 0.652 and val accuracy 0.779\n",
      "0.8492462311557789\n",
      "train loss 18.326 val loss 7.869 and val accuracy 0.764\n",
      "train loss 3.990 val loss 3.607 and val accuracy 0.754\n",
      "train loss 3.923 val loss 1.777 and val accuracy 0.829\n",
      "train loss 3.566 val loss 4.445 and val accuracy 0.764\n",
      "train loss 3.809 val loss 2.689 and val accuracy 0.714\n",
      "train loss 2.872 val loss 3.694 and val accuracy 0.794\n",
      "0.8291457286432161\n",
      "train loss 0.541 val loss 0.497 and val accuracy 0.804\n",
      "train loss 0.405 val loss 0.449 and val accuracy 0.859\n",
      "train loss 0.333 val loss 0.550 and val accuracy 0.839\n",
      "train loss 0.290 val loss 0.582 and val accuracy 0.804\n",
      "train loss 0.232 val loss 0.608 and val accuracy 0.809\n",
      "train loss 0.223 val loss 0.643 and val accuracy 0.809\n",
      "0.8592964824120602\n",
      "train loss 6.715 val loss 1.419 and val accuracy 0.779\n",
      "train loss 0.784 val loss 1.010 and val accuracy 0.633\n",
      "train loss 0.596 val loss 1.162 and val accuracy 0.784\n",
      "train loss 0.565 val loss 0.997 and val accuracy 0.769\n",
      "train loss 0.544 val loss 0.668 and val accuracy 0.774\n",
      "train loss 0.666 val loss 0.940 and val accuracy 0.683\n",
      "0.8090452261306532\n",
      "train loss 0.560 val loss 0.493 and val accuracy 0.804\n",
      "train loss 0.392 val loss 0.484 and val accuracy 0.819\n",
      "train loss 0.342 val loss 0.567 and val accuracy 0.794\n",
      "train loss 0.255 val loss 0.516 and val accuracy 0.824\n",
      "train loss 0.223 val loss 0.533 and val accuracy 0.829\n",
      "train loss 0.169 val loss 0.615 and val accuracy 0.814\n",
      "0.8341708542713567\n",
      "train loss 19.548 val loss 8.807 and val accuracy 0.769\n",
      "train loss 4.188 val loss 1.738 and val accuracy 0.809\n",
      "train loss 4.303 val loss 2.685 and val accuracy 0.804\n",
      "train loss 3.253 val loss 1.508 and val accuracy 0.804\n",
      "train loss 3.319 val loss 1.753 and val accuracy 0.849\n",
      "train loss 1.452 val loss 1.376 and val accuracy 0.814\n",
      "0.8492462311557789\n"
     ]
    }
   ],
   "source": [
    "for i in range(4): \n",
    "    vocab_size = len(word_idx_dict)\n",
    "    model = LSTMModel(vocab_size, 50, 50*(i+1), glove_weights = pretrained_weight)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.01)\n",
    "    max_acc = 0\n",
    "    train_epocs(model, optimizer, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)\n",
    "    \n",
    "    vocab_size = len(word_idx_dict)\n",
    "    model = LSTMModel(vocab_size, 50, 50*(i+1), glove_weights = pretrained_weight)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.1)\n",
    "    max_acc = 0\n",
    "    train_epocs(model, optimizer, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.560 val loss 0.512 and val accuracy 0.804\n",
      "train loss 0.135 val loss 0.670 and val accuracy 0.809\n",
      "train loss 0.090 val loss 0.594 and val accuracy 0.819\n",
      "train loss 0.064 val loss 0.818 and val accuracy 0.829\n",
      "train loss 0.060 val loss 0.754 and val accuracy 0.814\n",
      "train loss 0.067 val loss 0.582 and val accuracy 0.839\n",
      "0.8542713567839196\n",
      "train loss 0.700 val loss 0.651 and val accuracy 0.779\n",
      "train loss 0.310 val loss 0.911 and val accuracy 0.749\n",
      "train loss 0.197 val loss 0.936 and val accuracy 0.784\n",
      "train loss 0.158 val loss 1.180 and val accuracy 0.764\n",
      "train loss 0.191 val loss 1.317 and val accuracy 0.784\n",
      "train loss 0.137 val loss 1.296 and val accuracy 0.779\n",
      "0.7889447236180904\n",
      "train loss 0.540 val loss 0.495 and val accuracy 0.784\n",
      "train loss 0.125 val loss 0.882 and val accuracy 0.819\n",
      "train loss 0.081 val loss 0.714 and val accuracy 0.824\n",
      "train loss 0.064 val loss 0.546 and val accuracy 0.854\n",
      "train loss 0.065 val loss 0.631 and val accuracy 0.844\n",
      "train loss 0.054 val loss 0.745 and val accuracy 0.829\n",
      "0.8542713567839196\n",
      "train loss 2.261 val loss 2.777 and val accuracy 0.759\n",
      "train loss 0.641 val loss 1.437 and val accuracy 0.749\n",
      "train loss 0.344 val loss 1.378 and val accuracy 0.789\n",
      "train loss 0.342 val loss 1.217 and val accuracy 0.734\n",
      "train loss 0.147 val loss 1.350 and val accuracy 0.804\n",
      "train loss 0.237 val loss 1.606 and val accuracy 0.749\n",
      "0.8090452261306532\n",
      "train loss 0.552 val loss 0.522 and val accuracy 0.794\n",
      "train loss 0.146 val loss 0.519 and val accuracy 0.814\n",
      "train loss 0.082 val loss 0.572 and val accuracy 0.829\n",
      "train loss 0.061 val loss 0.691 and val accuracy 0.819\n",
      "train loss 0.067 val loss 0.732 and val accuracy 0.844\n",
      "train loss 0.067 val loss 0.680 and val accuracy 0.819\n",
      "0.8542713567839196\n",
      "train loss 3.900 val loss 2.179 and val accuracy 0.724\n",
      "train loss 0.606 val loss 1.536 and val accuracy 0.683\n",
      "train loss 0.631 val loss 2.778 and val accuracy 0.513\n",
      "train loss 0.528 val loss 2.296 and val accuracy 0.759\n",
      "train loss 0.291 val loss 2.196 and val accuracy 0.779\n",
      "train loss 0.471 val loss 4.011 and val accuracy 0.658\n",
      "0.8140703517587939\n",
      "train loss 0.665 val loss 0.679 and val accuracy 0.754\n",
      "train loss 0.236 val loss 0.656 and val accuracy 0.799\n",
      "train loss 0.083 val loss 0.684 and val accuracy 0.794\n",
      "train loss 0.066 val loss 0.641 and val accuracy 0.814\n",
      "train loss 0.067 val loss 0.641 and val accuracy 0.814\n",
      "train loss 0.060 val loss 0.750 and val accuracy 0.809\n",
      "0.8341708542713567\n",
      "train loss 3.868 val loss 7.616 and val accuracy 0.759\n",
      "train loss 0.632 val loss 1.743 and val accuracy 0.789\n",
      "train loss 0.410 val loss 1.810 and val accuracy 0.729\n",
      "train loss 0.298 val loss 1.887 and val accuracy 0.754\n",
      "train loss 0.261 val loss 1.939 and val accuracy 0.774\n",
      "train loss 0.324 val loss 1.978 and val accuracy 0.789\n",
      "0.8090452261306532\n"
     ]
    }
   ],
   "source": [
    "for i in range(4): \n",
    "    vocab_size = len(word_idx_dict)\n",
    "    model = LSTMModel(vocab_size, 50, 50*(i+1), glove_weights = pretrained_weight)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.01)\n",
    "    max_acc = 0\n",
    "    train_epocs(model, optimizer, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)\n",
    "    \n",
    "    vocab_size = len(word_idx_dict)\n",
    "    model = LSTMModel(vocab_size, 50, 50*(i+1), glove_weights = pretrained_weight)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.1)\n",
    "    max_acc = 0\n",
    "    train_epocs(model, optimizer, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### two lenear layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel1(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, hidden_dim1, glove_weights=None) :\n",
    "        super(LSTMModel1,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if glove_weights is not None:\n",
    "            self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "            self.embeddings.weight.requires_grad = True ## freeze embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim1)\n",
    "        self.linear1 = nn.Linear(hidden_dim1, 3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        out_pack, ht = self.lstm(x)\n",
    "        x = self.linear(ht[-1])\n",
    "        x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.512 val loss 0.484 and val accuracy 0.799\n",
      "train loss 0.156 val loss 0.657 and val accuracy 0.814\n",
      "train loss 0.089 val loss 0.800 and val accuracy 0.794\n",
      "train loss 0.080 val loss 0.682 and val accuracy 0.824\n",
      "train loss 0.065 val loss 0.782 and val accuracy 0.784\n",
      "train loss 0.062 val loss 0.792 and val accuracy 0.809\n",
      "0.8341708542713567\n",
      "train loss 1.379 val loss 0.972 and val accuracy 0.764\n",
      "train loss 0.524 val loss 0.762 and val accuracy 0.754\n",
      "train loss 0.693 val loss 1.230 and val accuracy 0.693\n",
      "train loss 0.478 val loss 1.159 and val accuracy 0.709\n",
      "train loss 0.546 val loss 2.760 and val accuracy 0.709\n",
      "train loss 0.639 val loss 1.275 and val accuracy 0.683\n",
      "0.7638190954773869\n",
      "train loss 0.495 val loss 0.490 and val accuracy 0.809\n",
      "train loss 0.158 val loss 0.493 and val accuracy 0.824\n",
      "train loss 0.078 val loss 0.522 and val accuracy 0.829\n",
      "train loss 0.078 val loss 0.711 and val accuracy 0.819\n",
      "train loss 0.066 val loss 0.721 and val accuracy 0.829\n",
      "train loss 0.066 val loss 0.575 and val accuracy 0.814\n",
      "0.8592964824120602\n",
      "train loss 71.657 val loss 77.688 and val accuracy 0.744\n",
      "train loss 0.961 val loss 1.119 and val accuracy 0.749\n",
      "train loss 0.713 val loss 1.902 and val accuracy 0.794\n",
      "train loss 0.363 val loss 1.456 and val accuracy 0.789\n",
      "train loss 2.860 val loss 7.952 and val accuracy 0.709\n",
      "train loss 1.308 val loss 4.139 and val accuracy 0.784\n",
      "0.8040201005025126\n",
      "train loss 0.570 val loss 0.528 and val accuracy 0.789\n",
      "train loss 0.176 val loss 0.525 and val accuracy 0.819\n",
      "train loss 0.098 val loss 0.719 and val accuracy 0.824\n",
      "train loss 0.067 val loss 0.748 and val accuracy 0.834\n",
      "train loss 0.078 val loss 0.642 and val accuracy 0.809\n",
      "train loss 0.064 val loss 0.578 and val accuracy 0.834\n",
      "0.8442211055276382\n",
      "train loss 13.212 val loss 5.027 and val accuracy 0.709\n",
      "train loss 0.567 val loss 1.432 and val accuracy 0.764\n",
      "train loss 0.262 val loss 1.244 and val accuracy 0.764\n",
      "train loss 0.200 val loss 0.920 and val accuracy 0.784\n",
      "train loss 0.128 val loss 1.075 and val accuracy 0.759\n",
      "train loss 0.123 val loss 1.012 and val accuracy 0.779\n",
      "0.8090452261306532\n",
      "train loss 0.560 val loss 0.520 and val accuracy 0.784\n",
      "train loss 0.221 val loss 0.558 and val accuracy 0.809\n",
      "train loss 0.121 val loss 0.534 and val accuracy 0.819\n",
      "train loss 0.074 val loss 0.698 and val accuracy 0.844\n",
      "train loss 0.087 val loss 0.633 and val accuracy 0.844\n",
      "train loss 0.072 val loss 0.704 and val accuracy 0.829\n",
      "0.8542713567839196\n",
      "train loss 11.959 val loss 7.020 and val accuracy 0.779\n",
      "train loss 3.214 val loss 1.683 and val accuracy 0.749\n",
      "train loss 0.734 val loss 1.266 and val accuracy 0.789\n",
      "train loss 0.769 val loss 1.792 and val accuracy 0.774\n",
      "train loss 0.436 val loss 1.592 and val accuracy 0.764\n",
      "train loss 0.335 val loss 0.847 and val accuracy 0.759\n",
      "0.8090452261306532\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    model1 = LSTMModel1(vocab_size, 50, 50*(i+1), 50, glove_weights = pretrained_weight)\n",
    "    parameters1 = filter(lambda p: p.requires_grad, model1.parameters())\n",
    "    optimizer1 = torch.optim.Adam(parameters1, lr=0.01)\n",
    "    max_acc = 0\n",
    "    train_epocs(model1, optimizer1, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)\n",
    "    \n",
    "    model1 = LSTMModel1(vocab_size, 50, 50*(i+1), 50, glove_weights = pretrained_weight)\n",
    "    parameters1 = filter(lambda p: p.requires_grad, model1.parameters())\n",
    "    optimizer1 = torch.optim.Adam(parameters1, lr=0.1)\n",
    "    max_acc = 0\n",
    "    train_epocs(model1, optimizer1, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.509 val loss 0.482 and val accuracy 0.794\n",
      "train loss 0.165 val loss 0.491 and val accuracy 0.804\n",
      "train loss 0.093 val loss 0.587 and val accuracy 0.819\n",
      "train loss 0.075 val loss 0.842 and val accuracy 0.824\n",
      "train loss 0.066 val loss 0.587 and val accuracy 0.849\n",
      "train loss 0.086 val loss 0.549 and val accuracy 0.834\n",
      "train loss 0.053 val loss 0.587 and val accuracy 0.829\n",
      "train loss 0.070 val loss 0.567 and val accuracy 0.839\n",
      "train loss 0.055 val loss 0.754 and val accuracy 0.859\n",
      "train loss 0.055 val loss 0.736 and val accuracy 0.864\n",
      "train loss 0.061 val loss 0.554 and val accuracy 0.834\n",
      "train loss 0.054 val loss 0.556 and val accuracy 0.834\n",
      "0.864321608040201\n",
      "train loss 0.961 val loss 1.116 and val accuracy 0.754\n",
      "train loss 0.407 val loss 0.695 and val accuracy 0.744\n",
      "train loss 0.311 val loss 0.868 and val accuracy 0.719\n",
      "train loss 0.301 val loss 1.058 and val accuracy 0.709\n",
      "train loss 0.441 val loss 3.917 and val accuracy 0.568\n",
      "train loss 0.453 val loss 1.518 and val accuracy 0.744\n",
      "train loss 2.223 val loss 5.607 and val accuracy 0.744\n",
      "train loss 1.035 val loss 3.179 and val accuracy 0.709\n",
      "train loss 1.183 val loss 4.432 and val accuracy 0.744\n",
      "train loss 2.949 val loss 6.954 and val accuracy 0.714\n",
      "train loss 1.478 val loss 5.309 and val accuracy 0.744\n",
      "train loss 1.934 val loss 4.467 and val accuracy 0.769\n",
      "0.7738693467336684\n",
      "train loss 0.542 val loss 0.519 and val accuracy 0.774\n",
      "train loss 0.180 val loss 0.700 and val accuracy 0.749\n",
      "train loss 0.090 val loss 0.692 and val accuracy 0.824\n",
      "train loss 0.078 val loss 0.716 and val accuracy 0.814\n",
      "train loss 0.074 val loss 0.778 and val accuracy 0.809\n",
      "train loss 0.058 val loss 1.039 and val accuracy 0.834\n",
      "train loss 0.058 val loss 1.003 and val accuracy 0.829\n",
      "train loss 0.067 val loss 1.053 and val accuracy 0.799\n",
      "train loss 0.064 val loss 1.167 and val accuracy 0.804\n",
      "train loss 0.064 val loss 0.816 and val accuracy 0.814\n",
      "train loss 0.057 val loss 0.854 and val accuracy 0.809\n",
      "train loss 0.067 val loss 0.875 and val accuracy 0.799\n",
      "0.8542713567839196\n",
      "train loss 5.419 val loss 5.870 and val accuracy 0.789\n",
      "train loss 0.691 val loss 0.753 and val accuracy 0.764\n",
      "train loss 0.356 val loss 0.716 and val accuracy 0.764\n",
      "train loss 0.277 val loss 0.720 and val accuracy 0.774\n",
      "train loss 0.259 val loss 0.729 and val accuracy 0.754\n",
      "train loss 0.244 val loss 0.823 and val accuracy 0.739\n",
      "train loss 0.213 val loss 0.734 and val accuracy 0.764\n",
      "train loss 0.162 val loss 0.979 and val accuracy 0.709\n",
      "train loss 0.181 val loss 0.940 and val accuracy 0.759\n",
      "train loss 0.162 val loss 1.020 and val accuracy 0.739\n",
      "train loss 0.172 val loss 0.943 and val accuracy 0.729\n",
      "train loss 0.155 val loss 1.301 and val accuracy 0.769\n",
      "0.7889447236180904\n",
      "train loss 0.628 val loss 0.647 and val accuracy 0.739\n",
      "train loss 0.194 val loss 0.651 and val accuracy 0.799\n",
      "train loss 0.080 val loss 0.642 and val accuracy 0.794\n",
      "train loss 0.076 val loss 0.598 and val accuracy 0.804\n",
      "train loss 0.072 val loss 0.906 and val accuracy 0.804\n",
      "train loss 0.062 val loss 0.805 and val accuracy 0.814\n",
      "train loss 0.063 val loss 0.658 and val accuracy 0.804\n",
      "train loss 0.061 val loss 0.831 and val accuracy 0.794\n",
      "train loss 0.069 val loss 0.690 and val accuracy 0.794\n",
      "train loss 0.061 val loss 0.755 and val accuracy 0.819\n",
      "train loss 0.062 val loss 0.698 and val accuracy 0.814\n",
      "train loss 0.062 val loss 0.994 and val accuracy 0.814\n",
      "0.8341708542713567\n",
      "train loss 16.767 val loss 6.960 and val accuracy 0.804\n",
      "train loss 1.170 val loss 1.062 and val accuracy 0.779\n",
      "train loss 0.486 val loss 0.784 and val accuracy 0.814\n",
      "train loss 0.511 val loss 0.941 and val accuracy 0.774\n",
      "train loss 0.322 val loss 0.814 and val accuracy 0.784\n",
      "train loss 0.412 val loss 1.183 and val accuracy 0.804\n",
      "train loss 0.442 val loss 2.185 and val accuracy 0.784\n",
      "train loss 0.363 val loss 0.886 and val accuracy 0.774\n",
      "train loss 0.271 val loss 0.904 and val accuracy 0.814\n",
      "train loss 0.179 val loss 0.646 and val accuracy 0.814\n",
      "train loss 0.190 val loss 0.709 and val accuracy 0.789\n",
      "train loss 0.320 val loss 1.399 and val accuracy 0.769\n",
      "0.8341708542713567\n",
      "train loss 0.656 val loss 0.562 and val accuracy 0.739\n",
      "train loss 0.191 val loss 0.688 and val accuracy 0.799\n",
      "train loss 0.112 val loss 0.551 and val accuracy 0.814\n",
      "train loss 0.085 val loss 0.657 and val accuracy 0.844\n",
      "train loss 0.062 val loss 0.812 and val accuracy 0.809\n",
      "train loss 0.064 val loss 0.718 and val accuracy 0.814\n",
      "train loss 0.061 val loss 0.628 and val accuracy 0.829\n",
      "train loss 0.064 val loss 0.646 and val accuracy 0.809\n",
      "train loss 0.057 val loss 0.737 and val accuracy 0.839\n",
      "train loss 0.079 val loss 0.881 and val accuracy 0.779\n",
      "train loss 0.055 val loss 0.977 and val accuracy 0.824\n",
      "train loss 0.060 val loss 0.930 and val accuracy 0.789\n",
      "0.8492462311557789\n",
      "train loss 32.013 val loss 30.346 and val accuracy 0.784\n",
      "train loss 1.785 val loss 2.750 and val accuracy 0.729\n",
      "train loss 0.598 val loss 1.320 and val accuracy 0.784\n",
      "train loss 2.359 val loss 8.916 and val accuracy 0.784\n",
      "train loss 135.738 val loss 674.276 and val accuracy 0.719\n",
      "train loss 12.401 val loss 79.920 and val accuracy 0.307\n",
      "train loss 4.100 val loss 15.342 and val accuracy 0.729\n",
      "train loss 4.511 val loss 35.803 and val accuracy 0.693\n",
      "train loss 3.292 val loss 9.029 and val accuracy 0.749\n",
      "train loss 1.789 val loss 6.539 and val accuracy 0.739\n",
      "train loss 1.313 val loss 8.562 and val accuracy 0.734\n",
      "train loss 2.044 val loss 7.376 and val accuracy 0.734\n",
      "0.7989949748743719\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    model1 = LSTMModel1(vocab_size, 50, 50*(i+1), 50, glove_weights = pretrained_weight)\n",
    "    parameters1 = filter(lambda p: p.requires_grad, model1.parameters())\n",
    "    optimizer1 = torch.optim.Adam(parameters1, lr=0.01)\n",
    "    max_acc = 0\n",
    "    train_epocs(model1, optimizer1, train_dl, valid_dl, epochs=60)\n",
    "    print(max_acc)\n",
    "    \n",
    "    model1 = LSTMModel1(vocab_size, 50, 50*(i+1), 50, glove_weights = pretrained_weight)\n",
    "    parameters1 = filter(lambda p: p.requires_grad, model1.parameters())\n",
    "    optimizer1 = torch.optim.Adam(parameters1, lr=0.1)\n",
    "    max_acc = 0\n",
    "    train_epocs(model1, optimizer1, train_dl, valid_dl, epochs=60)\n",
    "    print(max_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.521 val loss 0.539 and val accuracy 0.799\n",
      "train loss 0.166 val loss 0.658 and val accuracy 0.794\n",
      "train loss 0.084 val loss 1.044 and val accuracy 0.804\n",
      "train loss 0.075 val loss 0.762 and val accuracy 0.794\n",
      "train loss 0.061 val loss 0.856 and val accuracy 0.799\n",
      "train loss 0.067 val loss 0.898 and val accuracy 0.809\n",
      "train loss 0.062 val loss 0.879 and val accuracy 0.819\n",
      "train loss 0.056 val loss 0.816 and val accuracy 0.814\n",
      "train loss 0.067 val loss 1.071 and val accuracy 0.834\n",
      "train loss 0.070 val loss 1.074 and val accuracy 0.824\n",
      "train loss 0.056 val loss 0.928 and val accuracy 0.819\n",
      "train loss 0.054 val loss 0.989 and val accuracy 0.829\n",
      "0.8341708542713567\n",
      "train loss 0.514 val loss 0.578 and val accuracy 0.739\n",
      "train loss 0.278 val loss 0.722 and val accuracy 0.784\n",
      "train loss 0.259 val loss 0.649 and val accuracy 0.774\n",
      "train loss 0.356 val loss 0.942 and val accuracy 0.794\n",
      "train loss 0.361 val loss 1.028 and val accuracy 0.784\n",
      "train loss 0.316 val loss 1.000 and val accuracy 0.799\n",
      "train loss 0.305 val loss 0.813 and val accuracy 0.784\n",
      "train loss 0.233 val loss 0.814 and val accuracy 0.794\n",
      "train loss 0.339 val loss 1.333 and val accuracy 0.759\n",
      "train loss 0.249 val loss 1.471 and val accuracy 0.256\n",
      "train loss 0.383 val loss 1.043 and val accuracy 0.774\n",
      "train loss 0.459 val loss 1.379 and val accuracy 0.744\n",
      "0.8341708542713567\n",
      "train loss 0.548 val loss 0.486 and val accuracy 0.784\n",
      "train loss 0.123 val loss 0.889 and val accuracy 0.804\n",
      "train loss 0.090 val loss 0.696 and val accuracy 0.844\n",
      "train loss 0.062 val loss 0.902 and val accuracy 0.834\n",
      "train loss 0.064 val loss 0.926 and val accuracy 0.809\n",
      "train loss 0.060 val loss 0.834 and val accuracy 0.834\n",
      "train loss 0.065 val loss 0.899 and val accuracy 0.824\n",
      "train loss 0.055 val loss 0.900 and val accuracy 0.819\n",
      "train loss 0.054 val loss 0.932 and val accuracy 0.814\n",
      "train loss 0.057 val loss 0.905 and val accuracy 0.834\n",
      "train loss 0.053 val loss 0.811 and val accuracy 0.799\n",
      "train loss 0.056 val loss 1.062 and val accuracy 0.824\n",
      "0.8592964824120602\n",
      "train loss 2.241 val loss 1.165 and val accuracy 0.749\n",
      "train loss 0.440 val loss 0.866 and val accuracy 0.794\n",
      "train loss 0.332 val loss 1.216 and val accuracy 0.709\n",
      "train loss 0.260 val loss 0.982 and val accuracy 0.789\n",
      "train loss 0.226 val loss 1.118 and val accuracy 0.799\n",
      "train loss 0.176 val loss 1.159 and val accuracy 0.749\n",
      "train loss 0.248 val loss 1.277 and val accuracy 0.749\n",
      "train loss 0.183 val loss 1.196 and val accuracy 0.789\n",
      "train loss 0.239 val loss 1.825 and val accuracy 0.779\n",
      "train loss 0.163 val loss 2.167 and val accuracy 0.779\n",
      "train loss 0.134 val loss 1.605 and val accuracy 0.754\n",
      "train loss 0.142 val loss 1.378 and val accuracy 0.779\n",
      "0.7989949748743719\n",
      "train loss 0.589 val loss 0.529 and val accuracy 0.764\n",
      "train loss 0.168 val loss 0.589 and val accuracy 0.804\n",
      "train loss 0.084 val loss 0.578 and val accuracy 0.834\n",
      "train loss 0.081 val loss 0.655 and val accuracy 0.804\n",
      "train loss 0.065 val loss 0.749 and val accuracy 0.799\n",
      "train loss 0.059 val loss 0.668 and val accuracy 0.804\n",
      "train loss 0.059 val loss 0.657 and val accuracy 0.829\n",
      "train loss 0.068 val loss 0.669 and val accuracy 0.809\n",
      "train loss 0.057 val loss 0.860 and val accuracy 0.824\n",
      "train loss 0.060 val loss 0.968 and val accuracy 0.814\n",
      "train loss 0.053 val loss 0.809 and val accuracy 0.824\n",
      "train loss 0.146 val loss 1.312 and val accuracy 0.784\n",
      "0.8341708542713567\n",
      "train loss 4.486 val loss 4.055 and val accuracy 0.744\n",
      "train loss 0.872 val loss 2.293 and val accuracy 0.759\n",
      "train loss 0.399 val loss 1.226 and val accuracy 0.779\n",
      "train loss 0.381 val loss 1.294 and val accuracy 0.754\n",
      "train loss 0.299 val loss 1.166 and val accuracy 0.764\n",
      "train loss 0.239 val loss 0.937 and val accuracy 0.759\n",
      "train loss 0.184 val loss 1.177 and val accuracy 0.724\n",
      "train loss 0.210 val loss 1.253 and val accuracy 0.658\n",
      "train loss 0.180 val loss 1.083 and val accuracy 0.749\n",
      "train loss 0.165 val loss 0.916 and val accuracy 0.769\n",
      "train loss 0.181 val loss 0.952 and val accuracy 0.754\n",
      "train loss 0.163 val loss 1.185 and val accuracy 0.764\n",
      "0.7839195979899497\n",
      "train loss 0.590 val loss 0.480 and val accuracy 0.789\n",
      "train loss 0.183 val loss 0.543 and val accuracy 0.824\n",
      "train loss 0.104 val loss 0.780 and val accuracy 0.789\n",
      "train loss 0.068 val loss 0.782 and val accuracy 0.814\n",
      "train loss 0.064 val loss 0.664 and val accuracy 0.799\n",
      "train loss 0.062 val loss 0.814 and val accuracy 0.779\n",
      "train loss 0.058 val loss 0.662 and val accuracy 0.814\n",
      "train loss 0.065 val loss 0.688 and val accuracy 0.789\n",
      "train loss 0.069 val loss 0.821 and val accuracy 0.789\n",
      "train loss 0.060 val loss 0.684 and val accuracy 0.804\n",
      "train loss 0.057 val loss 0.679 and val accuracy 0.804\n",
      "train loss 0.055 val loss 0.796 and val accuracy 0.814\n",
      "0.8291457286432161\n",
      "train loss 9.152 val loss 4.559 and val accuracy 0.789\n",
      "train loss 2.044 val loss 3.043 and val accuracy 0.744\n",
      "train loss 2.695 val loss 6.029 and val accuracy 0.714\n",
      "train loss 0.934 val loss 4.340 and val accuracy 0.754\n",
      "train loss 2.395 val loss 6.560 and val accuracy 0.683\n",
      "train loss 0.969 val loss 3.840 and val accuracy 0.779\n",
      "train loss 1.327 val loss 5.036 and val accuracy 0.744\n",
      "train loss 1.498 val loss 6.113 and val accuracy 0.804\n",
      "train loss 1.523 val loss 4.589 and val accuracy 0.779\n",
      "train loss 1.403 val loss 4.632 and val accuracy 0.774\n",
      "train loss 1.405 val loss 4.960 and val accuracy 0.774\n",
      "train loss 1.287 val loss 6.856 and val accuracy 0.749\n",
      "0.8341708542713567\n"
     ]
    }
   ],
   "source": [
    "for i in range(4): \n",
    "    vocab_size = len(word_idx_dict)\n",
    "    model = LSTMModel(vocab_size, 50, 50*(i+1), glove_weights = pretrained_weight)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.01)\n",
    "    max_acc = 0\n",
    "    train_epocs(model, optimizer, train_dl, valid_dl, epochs=60)\n",
    "    print(max_acc)\n",
    "    \n",
    "    vocab_size = len(word_idx_dict)\n",
    "    model = LSTMModel(vocab_size, 50, 50*(i+1), glove_weights = pretrained_weight)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.1)\n",
    "    max_acc = 0\n",
    "    train_epocs(model, optimizer, train_dl, valid_dl, epochs=60)\n",
    "    print(max_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decreasing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.686 val loss 0.679 and val accuracy 0.749\n",
      "train loss 0.460 val loss 0.750 and val accuracy 0.719\n",
      "train loss 0.299 val loss 0.728 and val accuracy 0.774\n",
      "train loss 0.301 val loss 0.813 and val accuracy 0.819\n",
      "train loss 0.279 val loss 0.940 and val accuracy 0.749\n",
      "train loss 0.234 val loss 1.322 and val accuracy 0.779\n",
      "train loss 0.170 val loss 1.008 and val accuracy 0.749\n",
      "train loss 0.156 val loss 1.004 and val accuracy 0.759\n",
      "train loss 0.149 val loss 0.990 and val accuracy 0.759\n",
      "train loss 0.155 val loss 0.995 and val accuracy 0.754\n",
      "train loss 0.126 val loss 0.959 and val accuracy 0.749\n",
      "train loss 0.166 val loss 0.994 and val accuracy 0.759\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(vocab_size, 50, 50, glove_weights = pretrained_weight)\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.1)\n",
    "max_acc = 0\n",
    "train_epocs(model, optimizer, train_dl, valid_dl, epochs=30)\n",
    "update_optimizer(optimizer, 0.005)\n",
    "train_epocs(model, optimizer, train_dl, valid_dl, epochs=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
