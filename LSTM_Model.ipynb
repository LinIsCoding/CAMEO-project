{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from spacy.symbols import ORTH\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file into DataFrame\n",
    "df = pd.read_csv('./CAMEO_IDEA_labeled_data.csv')\n",
    "\n",
    "# separate content and label\n",
    "text = df['Content']\n",
    "labels = df['Category Code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion tokenize sentence\n",
    "tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = tokenizer.Defaults.stop_words\n",
    "# tokenize, lemmatize the text, drop punctuations and stopwords\n",
    "tokenize = lambda t: [token.lemma_ for token in tokenizer(t) if (not token.is_punct) and (not token.is_stop)]\n",
    "\n",
    "# only tokenize the text\n",
    "# tokenize = lambda t: [token.text for token in tokenizer(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary <key=word : value=count>\n",
    "cnt = Counter()\n",
    "size = text.size\n",
    "for idx in range(size):\n",
    "    for word in tokenize(text[idx]):\n",
    "        cnt[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out low-frequency word\n",
    "min_threshold = 1\n",
    "count = {x: count for x, count in cnt.items() if count >= min_threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out high-frequency word\n",
    "min_threshold = 1\n",
    "count = {x: count for x, count in cnt.items() if count <= min_threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(text)\n",
    "y = np.array(labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download glove dictionary\n",
    "# def download_glove():\n",
    "#     ! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#     ! unzip glove.6B.zip -C data\n",
    "    \n",
    "# download_glove()\n",
    "# ! unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embedding dictionary (<key=word : value=vector>)\n",
    "def load_embedding_dict():\n",
    "    embeddings_dict = {}\n",
    "    with open(\"glove.6B.50d.txt\", 'r') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "glove_dic = load_embedding_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries(<key=word : value=index number>) (<key=word : value=vector>)\n",
    "def create_embedding_matrix(count,emb_size=50):\n",
    "    size = len(count) + 2\n",
    "    word_idx_dict = {}\n",
    "    word_vec = np.zeros((size, emb_size), dtype=\"float32\")\n",
    "    \n",
    "    # add padding and UNK keyword\n",
    "    word_idx_dict[\"\"] = 0\n",
    "    word_vec[0] = np.zeros(emb_size, dtype='float32')\n",
    "    word_idx_dict[\"UNK\"] = 1\n",
    "    word_vec[1] = np.random.uniform(-0.25, 0.25, emb_size)\n",
    "\n",
    "    for i, word in enumerate(count.keys()):\n",
    "        word_idx_dict[word] = i + 2\n",
    "\n",
    "        if word in glove_dic:\n",
    "            word_vec[i + 2] = glove_dic[word]\n",
    "        else:\n",
    "            word_vec[i + 2] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "\n",
    "    return word_idx_dict, word_vec\n",
    "    \n",
    "word_idx_dict, pretrained_weight = create_embedding_matrix(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for encoding sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(line, word_idx_dict, N=400, padding_start=True):\n",
    "    tokens = tokenize(line)\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([word_idx_dict.get(word, word_idx_dict[\"UNK\"]) for word in tokens])\n",
    "    length = min(N, len(enc1))\n",
    "    if padding_start:\n",
    "        enc[:length] = enc1[:length]\n",
    "    else:\n",
    "        enc[N - length:] = enc1[:length]\n",
    "    return enc, length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DataSet and DataLoader for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventDataset(Dataset):\n",
    "    def __init__(self, X, y, N=40, padding_start=False):\n",
    "        self.y = y\n",
    "        self.X = [encode_sentence(line, word_idx_dict, N, padding_start) for line in X]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, s = self.X[idx]\n",
    "        return x, s, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = EventDataset(X_train, y_train)\n",
    "valid_ds = EventDataset(X_val, y_val)\n",
    "train_dl = DataLoader(train_ds, batch_size=30, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_optimizer(optimizer, lr):\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epocs(model, optimizer, train_dl, val_dl, epochs=10):\n",
    "    global max_acc\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, s, y in train_dl:\n",
    "            # s is not used in this model\n",
    "            x = x.long() \n",
    "            y = y.float() \n",
    "#             x = x.long()\n",
    "#             y = y.float()\n",
    "            y_pred = model(x)\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "#             print(y_pred.squeeze(0).size())\n",
    "            loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(0), y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc = val_metrics(model, val_dl)\n",
    "        if val_acc > max_acc:\n",
    "            max_acc = val_acc\n",
    "        if i % 5 == 1:\n",
    "            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for x, s, y in valid_dl:\n",
    "        # s is not used here\n",
    "        x = x.long()\n",
    "        y = y.float().unsqueeze(1)\n",
    "#         x = x.long()\n",
    "#         y = y.float().unsqueeze(1)\n",
    "        y_hat = model(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat.squeeze(0), y)\n",
    "        y_pred = y_hat > 0\n",
    "        correct += (y_pred.float() == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights=None) :\n",
    "        super(LSTMModel,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if glove_weights is not None:\n",
    "            self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "            self.embeddings.weight.requires_grad = True ## freeze embeddings\n",
    "            \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "#         print(x.size())\n",
    "        out_pack, ht = self.lstm(x)\n",
    "        x = self.linear(ht[-1])\n",
    "#         print(x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.288 val loss 0.232 and val accuracy 0.899\n",
      "train loss 0.062 val loss 0.314 and val accuracy 0.892\n",
      "train loss 0.077 val loss 0.228 and val accuracy 0.924\n",
      "train loss 0.027 val loss 0.298 and val accuracy 0.930\n",
      "train loss 0.027 val loss 0.303 and val accuracy 0.937\n",
      "train loss 0.024 val loss 0.307 and val accuracy 0.930\n",
      "tensor(0.9367)\n",
      "train loss 0.256 val loss 0.268 and val accuracy 0.886\n",
      "train loss 0.114 val loss 0.313 and val accuracy 0.861\n",
      "train loss 0.053 val loss 0.417 and val accuracy 0.867\n",
      "train loss 0.047 val loss 0.488 and val accuracy 0.829\n",
      "train loss 0.042 val loss 0.475 and val accuracy 0.835\n",
      "train loss 0.049 val loss 0.583 and val accuracy 0.842\n",
      "tensor(0.8861)\n",
      "train loss 0.312 val loss 0.258 and val accuracy 0.899\n",
      "train loss 0.051 val loss 0.307 and val accuracy 0.886\n",
      "train loss 0.032 val loss 0.423 and val accuracy 0.899\n",
      "train loss 0.026 val loss 0.379 and val accuracy 0.905\n",
      "train loss 0.033 val loss 0.331 and val accuracy 0.886\n",
      "train loss 0.025 val loss 0.456 and val accuracy 0.867\n",
      "tensor(0.9114)\n",
      "train loss 0.560 val loss 0.464 and val accuracy 0.899\n",
      "train loss 0.225 val loss 0.468 and val accuracy 0.854\n",
      "train loss 0.122 val loss 0.514 and val accuracy 0.835\n",
      "train loss 0.078 val loss 0.729 and val accuracy 0.873\n",
      "train loss 0.058 val loss 0.707 and val accuracy 0.829\n",
      "train loss 0.078 val loss 0.817 and val accuracy 0.835\n",
      "tensor(0.9114)\n",
      "train loss 0.313 val loss 0.313 and val accuracy 0.899\n",
      "train loss 0.085 val loss 0.364 and val accuracy 0.911\n",
      "train loss 0.035 val loss 0.296 and val accuracy 0.924\n",
      "train loss 0.027 val loss 0.358 and val accuracy 0.918\n",
      "train loss 0.023 val loss 0.388 and val accuracy 0.911\n",
      "train loss 0.024 val loss 0.331 and val accuracy 0.918\n",
      "tensor(0.9367)\n",
      "train loss 10.673 val loss 23.985 and val accuracy 0.101\n",
      "train loss 0.423 val loss 1.311 and val accuracy 0.905\n",
      "train loss 0.181 val loss 3.019 and val accuracy 0.899\n",
      "train loss 0.099 val loss 0.427 and val accuracy 0.899\n",
      "train loss 0.049 val loss 0.466 and val accuracy 0.911\n",
      "train loss 0.321 val loss 1.362 and val accuracy 0.899\n",
      "tensor(0.9114)\n",
      "train loss 0.323 val loss 0.313 and val accuracy 0.899\n",
      "train loss 0.117 val loss 0.221 and val accuracy 0.899\n",
      "train loss 0.038 val loss 0.292 and val accuracy 0.905\n",
      "train loss 0.033 val loss 0.353 and val accuracy 0.911\n",
      "train loss 0.024 val loss 0.441 and val accuracy 0.918\n",
      "train loss 0.032 val loss 0.449 and val accuracy 0.905\n",
      "tensor(0.9304)\n",
      "train loss 19.456 val loss 1.353 and val accuracy 0.899\n",
      "train loss 0.818 val loss 1.653 and val accuracy 0.899\n",
      "train loss 0.362 val loss 3.344 and val accuracy 0.899\n",
      "train loss 3.083 val loss 15.583 and val accuracy 0.899\n",
      "train loss 1.446 val loss 8.600 and val accuracy 0.899\n",
      "train loss 0.610 val loss 12.941 and val accuracy 0.899\n",
      "tensor(0.9051)\n"
     ]
    }
   ],
   "source": [
    "for i in range(4): \n",
    "    vocab_size = len(word_idx_dict)\n",
    "    model = LSTMModel(vocab_size, 50, 50*(i+1), glove_weights = pretrained_weight)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.01)\n",
    "    max_acc = 0\n",
    "    train_epocs(model, optimizer, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)\n",
    "    \n",
    "    vocab_size = len(word_idx_dict)\n",
    "    model = LSTMModel(vocab_size, 50, 50*(i+1), glove_weights = pretrained_weight)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.1)\n",
    "    max_acc = 0\n",
    "    train_epocs(model, optimizer, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel1(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, hidden_dim1, glove_weights=None) :\n",
    "        super(LSTMModel1,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if glove_weights is not None:\n",
    "            self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "            self.embeddings.weight.requires_grad = True ## freeze embeddings\n",
    "            \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim1)\n",
    "        self.linear1 = nn.Linear(hidden_dim1, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "#         print(x.size())\n",
    "        out_pack, ht = self.lstm(x)\n",
    "        x = self.linear(ht[-1])\n",
    "        x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.245 val loss 0.287 and val accuracy 0.911\n",
      "train loss 0.054 val loss 0.273 and val accuracy 0.918\n",
      "train loss 0.034 val loss 0.366 and val accuracy 0.924\n",
      "train loss 0.033 val loss 0.338 and val accuracy 0.918\n",
      "train loss 0.033 val loss 0.301 and val accuracy 0.918\n",
      "train loss 0.029 val loss 0.419 and val accuracy 0.924\n",
      "tensor(0.9304)\n",
      "train loss 0.408 val loss 0.319 and val accuracy 0.899\n",
      "train loss 0.131 val loss 0.328 and val accuracy 0.899\n",
      "train loss 0.087 val loss 0.368 and val accuracy 0.911\n",
      "train loss 0.069 val loss 0.297 and val accuracy 0.911\n",
      "train loss 0.061 val loss 0.432 and val accuracy 0.899\n",
      "train loss 0.096 val loss 0.340 and val accuracy 0.924\n",
      "tensor(0.9241)\n",
      "train loss 0.260 val loss 0.270 and val accuracy 0.911\n",
      "train loss 0.069 val loss 0.319 and val accuracy 0.918\n",
      "train loss 0.048 val loss 0.384 and val accuracy 0.911\n",
      "train loss 0.052 val loss 0.327 and val accuracy 0.905\n",
      "train loss 0.034 val loss 0.382 and val accuracy 0.911\n",
      "train loss 0.033 val loss 0.294 and val accuracy 0.911\n",
      "tensor(0.9241)\n",
      "train loss 0.642 val loss 0.555 and val accuracy 0.867\n",
      "train loss 0.198 val loss 0.380 and val accuracy 0.899\n",
      "train loss 0.121 val loss 0.962 and val accuracy 0.892\n",
      "train loss 0.123 val loss 0.754 and val accuracy 0.899\n",
      "train loss 3.857 val loss 10.709 and val accuracy 0.899\n",
      "train loss 0.675 val loss 1.375 and val accuracy 0.899\n",
      "tensor(0.9114)\n",
      "train loss 0.241 val loss 0.281 and val accuracy 0.911\n",
      "train loss 0.068 val loss 0.295 and val accuracy 0.943\n",
      "train loss 0.037 val loss 0.325 and val accuracy 0.930\n",
      "train loss 0.036 val loss 0.410 and val accuracy 0.918\n",
      "train loss 0.049 val loss 0.280 and val accuracy 0.930\n",
      "train loss 0.050 val loss 0.223 and val accuracy 0.930\n",
      "tensor(0.9430)\n",
      "train loss 2.825 val loss 0.315 and val accuracy 0.892\n",
      "train loss 0.166 val loss 0.496 and val accuracy 0.899\n",
      "train loss 0.188 val loss 0.438 and val accuracy 0.899\n",
      "train loss 0.187 val loss 0.362 and val accuracy 0.873\n",
      "train loss 0.410 val loss 0.477 and val accuracy 0.899\n",
      "train loss 0.371 val loss 1.807 and val accuracy 0.899\n",
      "tensor(0.9114)\n",
      "train loss 0.280 val loss 0.266 and val accuracy 0.899\n",
      "train loss 0.081 val loss 0.280 and val accuracy 0.924\n",
      "train loss 0.047 val loss 0.367 and val accuracy 0.911\n",
      "train loss 0.050 val loss 0.297 and val accuracy 0.899\n",
      "train loss 0.033 val loss 0.268 and val accuracy 0.899\n",
      "train loss 0.040 val loss 0.314 and val accuracy 0.905\n",
      "tensor(0.9304)\n",
      "train loss 2.665 val loss 0.819 and val accuracy 0.886\n",
      "train loss 0.142 val loss 0.470 and val accuracy 0.886\n",
      "train loss 0.073 val loss 0.450 and val accuracy 0.892\n",
      "train loss 0.101 val loss 0.480 and val accuracy 0.886\n",
      "train loss 0.058 val loss 0.437 and val accuracy 0.892\n",
      "train loss 0.072 val loss 0.346 and val accuracy 0.892\n",
      "tensor(0.8987)\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    model1 = LSTMModel1(vocab_size, 50, 50*(i+1), 50, glove_weights = pretrained_weight)\n",
    "    parameters1 = filter(lambda p: p.requires_grad, model1.parameters())\n",
    "    optimizer1 = torch.optim.Adam(parameters1, lr=0.01)\n",
    "    max_acc = 0\n",
    "    train_epocs(model1, optimizer1, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)\n",
    "    \n",
    "    model1 = LSTMModel1(vocab_size, 50, 50*(i+1), 50, glove_weights = pretrained_weight)\n",
    "    parameters1 = filter(lambda p: p.requires_grad, model1.parameters())\n",
    "    optimizer1 = torch.optim.Adam(parameters1, lr=0.1)\n",
    "    max_acc = 0\n",
    "    train_epocs(model1, optimizer1, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
