{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file into DataFrame\n",
    "df = pd.read_csv('./CAMEO_IDEA_labeled_data.csv')\n",
    "\n",
    "# separate content and label\n",
    "text = df['Content']\n",
    "labels = df['Category Code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion tokenize sentence\n",
    "tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "# tokenize, lemmatize the text and drop punctuations\n",
    "tokenize = lambda t: [token.lemma_ for token in tokenizer(t) if not token.is_punct]\n",
    "\n",
    "# only tokenize the text\n",
    "#tokenize = lambda t: [token.text for token in tokenizer(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary <key=word : value=count>\n",
    "cnt = Counter()\n",
    "size = text.size\n",
    "for idx in range(size):\n",
    "    for word in tokenize(text[idx]):\n",
    "        cnt[word] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out low-frequency word\n",
    "min_threshold = 1\n",
    "count = {x: count for x, count in cnt.items() if count >= min_threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out high-frequency word\n",
    "max_threshold = 100\n",
    "count = {x: count for x, count in cnt.items() if count <= max_threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(text)\n",
    "y = np.array(labels)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download glove dictionary\n",
    "# def download_glove():\n",
    "#     ! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#     ! unzip glove.6B.zip -C data\n",
    "    \n",
    "# download_glove()\n",
    "# ! unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embedding dictionary (<key=word : value=vector>)\n",
    "# word2vec dictionary\n",
    "def load_embedding_dict():\n",
    "    embeddings_dict = {}\n",
    "    with open(\"glove.6B.50d.txt\", 'r') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "glove_dic = load_embedding_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code is implemented by Yudan Su"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert each event  into word vectors by  averaging the word embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the setences into a big matrix.\n",
    "# m * d: m is the number of sentences; d is the embed size. \n",
    "def get_feature_matrix(X, glove_dic):\n",
    "    sentences_matrix = []\n",
    "    for line in X:\n",
    "        sentences_matrix.append(sentence2vector(line, glove_dic))\n",
    "    return np.array(sentences_matrix)\n",
    "\n",
    "    \n",
    "# x is a setence of words, convert it into an embedding vector of dim size\n",
    "# by averaging the word embedding vectors (column-wise). \n",
    "def sentence2vector(x, glove_dic): \n",
    "    word_list = tokenize(x)\n",
    "    word_matrix = []\n",
    "    for word in word_list:\n",
    "        if word in glove_dic:\n",
    "            word_matrix.append(glove_dic[word])\n",
    "    word_matrix = np.array(word_matrix)\n",
    "    sentence_vector = np.mean(word_matrix, axis=0)\n",
    "    return sentence_vector   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(786,)\n",
      "Counter({0: 705, 1: 81})\n",
      "Liner classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9113924050632911\n",
      "f1_weighted:  0.8866062264796443\n",
      "MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9050632911392406\n",
      "f1_weighted:  0.8929140023489496\n",
      "SVM...\n",
      "accuracy:  0.8987341772151899\n",
      "f1_weighted:  0.8508016877637131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import collections\n",
    "\n",
    "# evaluate MLP model on the dataset\n",
    "# print metrics of accuracy, precision, and F1-score\n",
    "def evaluate_model(X, y, model=LogisticRegression(random_state=0)):\n",
    "    X = get_feature_matrix(X, glove_dic)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "    clf = model.fit(train_x, train_y)\n",
    "    predict_y = clf.predict(test_x)\n",
    "    accuracy = accuracy_score(test_y, predict_y)\n",
    "#     precision = precision_score(test_y, predict_y)\n",
    "#     f1score = f1_score(test_y, predict_y)\n",
    "#     recall = recall_score(test_y, predict_y)\n",
    "    f1_weighted = f1_score(test_y, predict_y, average='weighted')\n",
    "    print(\"accuracy: \", accuracy)\n",
    "    print(\"f1_weighted: \", f1_weighted)\n",
    "#     print (\"test_y: \",test_y)\n",
    "#     print (\"predict_y; \", predict_y)\n",
    "        \n",
    "print (X.shape)\n",
    "print (collections.Counter(y))\n",
    "print (\"Liner classifier...\")\n",
    "evaluate_model(X, y)\n",
    "print (\"MLP...\")\n",
    "evaluate_model(X, y, model=MLPClassifier(hidden_layer_sizes=(64,),random_state=1, max_iter=1000))\n",
    "print (\"SVM...\")\n",
    "evaluate_model(X, y, model=SVC())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
