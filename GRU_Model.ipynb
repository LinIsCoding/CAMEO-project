{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from spacy.symbols import ORTH\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file into DataFrame\n",
    "df = pd.read_csv('./CAMEO_IDEA_labeled_data.csv')\n",
    "\n",
    "# separate content and label\n",
    "text = df['Content']\n",
    "labels = df['Category Code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion tokenize sentence\n",
    "tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = tokenizer.Defaults.stop_words\n",
    "# tokenize, lemmatize the text, drop punctuations and stopwords\n",
    "tokenize = lambda t: [token.lemma_ for token in tokenizer(t) if (not token.is_punct) and (not token.is_stop)]\n",
    "\n",
    "# only tokenize the text\n",
    "# tokenize = lambda t: [token.text for token in tokenizer(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary <key=word : value=count>\n",
    "cnt = Counter()\n",
    "size = text.size\n",
    "for idx in range(size):\n",
    "    for word in tokenize(text[idx]):\n",
    "        cnt[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = dict(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out low-frequency word\n",
    "min_threshold = 1\n",
    "count = {x: count for x, count in cnt.items() if count >= min_threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out high-frequency word\n",
    "max_threshold = 1\n",
    "count = {x: count for x, count in cnt.items() if count <= max_threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(text)\n",
    "y = np.array(labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download glove dictionary\n",
    "# def download_glove():\n",
    "#     ! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#     ! unzip glove.6B.zip -C data\n",
    "    \n",
    "# download_glove()\n",
    "# ! unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embedding dictionary (<key=word : value=vector>)\n",
    "def load_embedding_dict():\n",
    "    embeddings_dict = {}\n",
    "    with open(\"glove.6B.50d.txt\", 'r') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "glove_dic = load_embedding_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries(<key=word : value=index number>) (<key=word : value=vector>)\n",
    "def create_embedding_matrix(count,emb_size=50):\n",
    "    size = len(count) + 2\n",
    "    word_idx_dict = {}\n",
    "    word_vec = np.zeros((size, emb_size), dtype=\"float32\")\n",
    "    \n",
    "    # add padding and UNK keyword\n",
    "    word_idx_dict[\"\"] = 0\n",
    "    word_vec[0] = np.zeros(emb_size, dtype='float32')\n",
    "    word_idx_dict[\"UNK\"] = 1\n",
    "    word_vec[1] = np.random.uniform(-0.25, 0.25, emb_size)\n",
    "\n",
    "    for i, word in enumerate(count.keys()):\n",
    "        word_idx_dict[word] = i + 2\n",
    "\n",
    "        if word in glove_dic:\n",
    "            word_vec[i + 2] = glove_dic[word]\n",
    "        else:\n",
    "            word_vec[i + 2] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "\n",
    "    return word_idx_dict, word_vec\n",
    "    \n",
    "word_idx_dict, pretrained_weight = create_embedding_matrix(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for encoding sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(line, word_idx_dict, N=400, padding_start=True):\n",
    "    tokens = tokenize(line)\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([word_idx_dict.get(word, word_idx_dict[\"UNK\"]) for word in tokens])\n",
    "    length = min(N, len(enc1))\n",
    "    if padding_start:\n",
    "        enc[:length] = enc1[:length]\n",
    "    else:\n",
    "        enc[N - length:] = enc1[:length]\n",
    "    return enc, length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DataSet and DataLoader for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventDataset(Dataset):\n",
    "    def __init__(self, X, y, N=40, padding_start=False):\n",
    "        self.y = y\n",
    "        self.X = [encode_sentence(line, word_idx_dict, N, padding_start) for line in X]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, s = self.X[idx]\n",
    "        return x, s, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = EventDataset(X_train, y_train)\n",
    "valid_ds = EventDataset(X_val, y_val)\n",
    "train_dl = DataLoader(train_ds, batch_size=30, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_optimizer(optimizer, lr):\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epocs(model, optimizer, train_dl, val_dl, epochs=10):\n",
    "    global max_acc\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, s, y in train_dl:\n",
    "            # s is not used in this model\n",
    "            x = x.long() \n",
    "            y = y.long() \n",
    "            y_pred = model(x)           \n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc = val_metrics(model, val_dl)\n",
    "        if val_acc > max_acc:\n",
    "            max_acc = val_acc\n",
    "        if i % 5 == 0:\n",
    "            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, val_dl):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    for x, s, y in val_dl:\n",
    "        x = x.long()  #.cuda()\n",
    "        y = y.long()\n",
    "        batch = y.shape[0]\n",
    "#         print(y.size())\n",
    "        out = model(x)\n",
    "        \n",
    "        loss = F.cross_entropy(out, y)\n",
    "        sum_loss += batch*(loss.item())\n",
    "        total += batch\n",
    "        _, pred = torch.max(out, 1) \n",
    "#         print(pred.size())\n",
    "        correct += (pred == y.data).float().sum().item()\n",
    "    val_loss = sum_loss/total\n",
    "    val_acc = correct/total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights=None) :\n",
    "        super(GRUModel,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if glove_weights is not None:\n",
    "            self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "            self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "            \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        out_pack, ht = self.gru(x)\n",
    "        x = self.linear(ht[-1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.677 val loss 0.609 and val accuracy 0.759\n",
      "train loss 0.457 val loss 0.498 and val accuracy 0.824\n",
      "train loss 0.387 val loss 0.459 and val accuracy 0.839\n",
      "train loss 0.309 val loss 0.483 and val accuracy 0.819\n",
      "train loss 0.267 val loss 0.537 and val accuracy 0.799\n",
      "train loss 0.275 val loss 0.531 and val accuracy 0.814\n",
      "train loss 0.225 val loss 0.580 and val accuracy 0.804\n",
      "train loss 0.246 val loss 0.603 and val accuracy 0.799\n",
      "train loss 0.229 val loss 0.665 and val accuracy 0.794\n",
      "train loss 0.226 val loss 0.615 and val accuracy 0.804\n",
      "train loss 0.231 val loss 0.641 and val accuracy 0.809\n",
      "train loss 0.188 val loss 0.551 and val accuracy 0.819\n",
      "0.8542713567839196\n",
      "train loss 0.909 val loss 0.701 and val accuracy 0.704\n",
      "train loss 0.802 val loss 0.655 and val accuracy 0.754\n",
      "train loss 0.620 val loss 0.793 and val accuracy 0.734\n",
      "train loss 0.824 val loss 0.723 and val accuracy 0.724\n",
      "train loss 0.723 val loss 0.661 and val accuracy 0.729\n",
      "train loss 0.765 val loss 0.953 and val accuracy 0.739\n",
      "train loss 0.788 val loss 0.777 and val accuracy 0.734\n",
      "train loss 0.804 val loss 0.836 and val accuracy 0.734\n",
      "train loss 0.728 val loss 0.789 and val accuracy 0.754\n",
      "train loss 0.730 val loss 0.717 and val accuracy 0.759\n",
      "train loss 0.909 val loss 0.814 and val accuracy 0.678\n",
      "train loss 0.752 val loss 0.772 and val accuracy 0.724\n",
      "0.7839195979899497\n",
      "train loss 0.661 val loss 0.575 and val accuracy 0.754\n",
      "train loss 0.430 val loss 0.465 and val accuracy 0.824\n",
      "train loss 0.348 val loss 0.516 and val accuracy 0.824\n",
      "train loss 0.274 val loss 0.574 and val accuracy 0.814\n",
      "train loss 0.258 val loss 0.609 and val accuracy 0.804\n",
      "train loss 0.243 val loss 0.691 and val accuracy 0.754\n",
      "train loss 0.237 val loss 0.665 and val accuracy 0.814\n",
      "train loss 0.197 val loss 0.726 and val accuracy 0.779\n",
      "train loss 0.224 val loss 0.652 and val accuracy 0.804\n",
      "train loss 0.204 val loss 0.783 and val accuracy 0.784\n",
      "train loss 0.187 val loss 0.747 and val accuracy 0.794\n",
      "train loss 0.201 val loss 0.916 and val accuracy 0.744\n",
      "0.8291457286432161\n",
      "train loss 1.299 val loss 0.996 and val accuracy 0.648\n",
      "train loss 0.786 val loss 0.680 and val accuracy 0.769\n",
      "train loss 0.706 val loss 0.976 and val accuracy 0.714\n",
      "train loss 0.776 val loss 1.300 and val accuracy 0.704\n",
      "train loss 0.740 val loss 0.705 and val accuracy 0.769\n",
      "train loss 0.800 val loss 1.172 and val accuracy 0.744\n",
      "train loss 1.118 val loss 2.003 and val accuracy 0.166\n",
      "train loss 0.810 val loss 0.853 and val accuracy 0.709\n",
      "train loss 1.213 val loss 1.250 and val accuracy 0.729\n",
      "train loss 0.984 val loss 0.870 and val accuracy 0.729\n",
      "train loss 0.909 val loss 0.987 and val accuracy 0.719\n",
      "train loss 0.944 val loss 0.875 and val accuracy 0.678\n",
      "0.7839195979899497\n",
      "train loss 0.685 val loss 0.587 and val accuracy 0.744\n",
      "train loss 0.415 val loss 0.496 and val accuracy 0.819\n",
      "train loss 0.353 val loss 0.535 and val accuracy 0.839\n",
      "train loss 0.280 val loss 0.587 and val accuracy 0.819\n",
      "train loss 0.238 val loss 0.614 and val accuracy 0.794\n",
      "train loss 0.259 val loss 0.661 and val accuracy 0.814\n",
      "train loss 0.220 val loss 0.640 and val accuracy 0.779\n",
      "train loss 0.214 val loss 0.720 and val accuracy 0.794\n",
      "train loss 0.264 val loss 0.692 and val accuracy 0.799\n",
      "train loss 0.232 val loss 0.708 and val accuracy 0.789\n",
      "train loss 0.231 val loss 0.750 and val accuracy 0.809\n",
      "train loss 0.325 val loss 0.673 and val accuracy 0.819\n",
      "0.8492462311557789\n",
      "train loss 1.861 val loss 1.300 and val accuracy 0.156\n",
      "train loss 0.981 val loss 0.665 and val accuracy 0.744\n",
      "train loss 0.726 val loss 1.536 and val accuracy 0.784\n",
      "train loss 1.163 val loss 1.271 and val accuracy 0.201\n",
      "train loss 0.803 val loss 1.067 and val accuracy 0.764\n",
      "train loss 1.054 val loss 0.818 and val accuracy 0.754\n",
      "train loss 0.757 val loss 0.791 and val accuracy 0.759\n",
      "train loss 0.680 val loss 0.839 and val accuracy 0.769\n",
      "train loss 0.949 val loss 0.942 and val accuracy 0.729\n",
      "train loss 0.908 val loss 1.047 and val accuracy 0.774\n",
      "train loss 0.737 val loss 0.988 and val accuracy 0.719\n",
      "train loss 0.932 val loss 0.920 and val accuracy 0.759\n",
      "0.7939698492462312\n",
      "train loss 0.705 val loss 0.597 and val accuracy 0.759\n",
      "train loss 0.421 val loss 0.481 and val accuracy 0.829\n",
      "train loss 0.328 val loss 0.473 and val accuracy 0.824\n",
      "train loss 0.256 val loss 0.535 and val accuracy 0.834\n",
      "train loss 0.266 val loss 0.567 and val accuracy 0.764\n",
      "train loss 0.245 val loss 0.571 and val accuracy 0.834\n",
      "train loss 0.296 val loss 0.597 and val accuracy 0.779\n",
      "train loss 0.245 val loss 0.574 and val accuracy 0.814\n",
      "train loss 0.230 val loss 0.504 and val accuracy 0.834\n",
      "train loss 0.213 val loss 0.575 and val accuracy 0.804\n",
      "train loss 0.223 val loss 0.686 and val accuracy 0.799\n",
      "train loss 0.278 val loss 0.643 and val accuracy 0.799\n",
      "0.8341708542713567\n",
      "train loss 2.175 val loss 1.413 and val accuracy 0.739\n",
      "train loss 0.974 val loss 0.816 and val accuracy 0.769\n",
      "train loss 0.787 val loss 0.941 and val accuracy 0.754\n",
      "train loss 0.958 val loss 1.190 and val accuracy 0.759\n",
      "train loss 0.812 val loss 1.202 and val accuracy 0.688\n",
      "train loss 1.826 val loss 1.764 and val accuracy 0.764\n",
      "train loss 1.464 val loss 1.834 and val accuracy 0.754\n",
      "train loss 1.252 val loss 1.378 and val accuracy 0.764\n",
      "train loss 1.112 val loss 0.806 and val accuracy 0.774\n",
      "train loss 1.010 val loss 1.645 and val accuracy 0.749\n",
      "train loss 0.853 val loss 0.800 and val accuracy 0.714\n",
      "train loss 0.855 val loss 0.868 and val accuracy 0.769\n",
      "0.7889447236180904\n"
     ]
    }
   ],
   "source": [
    "for i in range(4): \n",
    "    vocab_size = len(word_idx_dict)\n",
    "    model = GRUModel(vocab_size, 50, 50*(i+1), glove_weights = pretrained_weight)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.01)\n",
    "    max_acc = 0\n",
    "    train_epocs(model, optimizer, train_dl, valid_dl, epochs=60)\n",
    "    print(max_acc)\n",
    "    \n",
    "    vocab_size = len(word_idx_dict)\n",
    "    model = GRUModel(vocab_size, 50, 50*(i+1), glove_weights = pretrained_weight)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.1)\n",
    "    max_acc = 0\n",
    "    train_epocs(model, optimizer, train_dl, valid_dl, epochs=60)\n",
    "    print(max_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decreasing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 2.593 val loss 1.238 and val accuracy 0.719\n",
      "train loss 0.598 val loss 0.486 and val accuracy 0.814\n",
      "train loss 0.608 val loss 0.690 and val accuracy 0.724\n",
      "train loss 0.653 val loss 0.570 and val accuracy 0.769\n",
      "train loss 0.832 val loss 0.779 and val accuracy 0.729\n",
      "train loss 0.807 val loss 0.647 and val accuracy 0.754\n",
      "train loss 0.694 val loss 0.774 and val accuracy 0.744\n",
      "train loss 0.547 val loss 0.667 and val accuracy 0.739\n",
      "train loss 0.520 val loss 0.638 and val accuracy 0.754\n",
      "train loss 0.502 val loss 0.668 and val accuracy 0.714\n",
      "train loss 0.523 val loss 0.849 and val accuracy 0.648\n",
      "train loss 0.516 val loss 0.762 and val accuracy 0.744\n"
     ]
    }
   ],
   "source": [
    "model = GRUModel1(vocab_size, 50, 50, 50, glove_weights = pretrained_weight)\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.1)\n",
    "max_acc = 0\n",
    "train_epocs(model, optimizer, train_dl, valid_dl, epochs=30)\n",
    "\n",
    "update_optimizer(optimizer, 0.005)\n",
    "train_epocs(model, optimizer, train_dl, valid_dl, epochs=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8241206030150754"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with two linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel1(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, hidden_dim1, glove_weights=None) :\n",
    "        super(GRUModel1,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if glove_weights is not None:\n",
    "            self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "            self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "            \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim1)\n",
    "        self.linear1 = nn.Linear(hidden_dim1, 3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        out_pack, ht = self.gru(x)\n",
    "        x = self.linear(ht[-1])\n",
    "        x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.704 val loss 0.526 and val accuracy 0.804\n",
      "train loss 0.447 val loss 0.468 and val accuracy 0.814\n",
      "train loss 0.376 val loss 0.480 and val accuracy 0.834\n",
      "train loss 0.326 val loss 0.612 and val accuracy 0.819\n",
      "train loss 0.309 val loss 0.519 and val accuracy 0.794\n",
      "train loss 0.284 val loss 0.502 and val accuracy 0.839\n",
      "0.8391959798994975\n",
      "train loss 2.369 val loss 0.861 and val accuracy 0.744\n",
      "train loss 0.680 val loss 0.678 and val accuracy 0.714\n",
      "train loss 0.980 val loss 1.079 and val accuracy 0.653\n",
      "train loss 3.440 val loss 1.982 and val accuracy 0.739\n",
      "train loss 0.842 val loss 0.928 and val accuracy 0.749\n",
      "train loss 0.767 val loss 0.737 and val accuracy 0.754\n",
      "0.7587939698492462\n",
      "train loss 0.738 val loss 0.568 and val accuracy 0.754\n",
      "train loss 0.437 val loss 0.504 and val accuracy 0.794\n",
      "train loss 0.360 val loss 0.509 and val accuracy 0.814\n",
      "train loss 0.306 val loss 0.616 and val accuracy 0.779\n",
      "train loss 0.271 val loss 0.666 and val accuracy 0.774\n",
      "train loss 0.225 val loss 0.816 and val accuracy 0.789\n",
      "0.8341708542713567\n",
      "train loss 4.190 val loss 2.998 and val accuracy 0.744\n",
      "train loss 0.850 val loss 1.118 and val accuracy 0.749\n",
      "train loss 1.664 val loss 1.509 and val accuracy 0.201\n",
      "train loss 0.712 val loss 0.953 and val accuracy 0.693\n",
      "train loss 0.668 val loss 0.702 and val accuracy 0.724\n",
      "train loss 0.687 val loss 0.736 and val accuracy 0.714\n",
      "0.7537688442211056\n",
      "train loss 0.757 val loss 0.606 and val accuracy 0.754\n",
      "train loss 0.449 val loss 0.495 and val accuracy 0.804\n",
      "train loss 0.381 val loss 0.494 and val accuracy 0.794\n",
      "train loss 0.354 val loss 0.562 and val accuracy 0.774\n",
      "train loss 0.230 val loss 0.656 and val accuracy 0.789\n",
      "train loss 0.240 val loss 0.537 and val accuracy 0.794\n",
      "0.8341708542713567\n",
      "train loss 8.876 val loss 1.209 and val accuracy 0.693\n",
      "train loss 0.712 val loss 0.759 and val accuracy 0.744\n",
      "train loss 0.676 val loss 0.650 and val accuracy 0.754\n",
      "train loss 0.651 val loss 0.692 and val accuracy 0.754\n",
      "train loss 5.736 val loss 2.706 and val accuracy 0.668\n",
      "train loss 0.875 val loss 1.085 and val accuracy 0.588\n",
      "0.7688442211055276\n",
      "train loss 0.831 val loss 0.675 and val accuracy 0.719\n",
      "train loss 0.456 val loss 0.488 and val accuracy 0.824\n",
      "train loss 0.351 val loss 0.501 and val accuracy 0.789\n",
      "train loss 0.321 val loss 0.519 and val accuracy 0.819\n",
      "train loss 0.301 val loss 0.529 and val accuracy 0.829\n",
      "train loss 0.270 val loss 0.617 and val accuracy 0.799\n",
      "0.8341708542713567\n",
      "train loss 10.990 val loss 3.498 and val accuracy 0.176\n",
      "train loss 0.770 val loss 0.683 and val accuracy 0.764\n",
      "train loss 0.645 val loss 0.666 and val accuracy 0.779\n",
      "train loss 0.600 val loss 0.592 and val accuracy 0.799\n",
      "train loss 28.754 val loss 74.356 and val accuracy 0.749\n",
      "train loss 1.730 val loss 1.209 and val accuracy 0.734\n",
      "0.8190954773869347\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    model1 = GRUModel1(vocab_size, 50, 50*(i+1), 50, glove_weights = pretrained_weight)\n",
    "    parameters1 = filter(lambda p: p.requires_grad, model1.parameters())\n",
    "    optimizer1 = torch.optim.Adam(parameters1, lr=0.01)\n",
    "    max_acc = 0\n",
    "    train_epocs(model1, optimizer1, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)\n",
    "    \n",
    "    model1 = GRUModel1(vocab_size, 50, 50*(i+1), 50, glove_weights = pretrained_weight)\n",
    "    parameters1 = filter(lambda p: p.requires_grad, model1.parameters())\n",
    "    optimizer1 = torch.optim.Adam(parameters1, lr=0.1)\n",
    "    max_acc = 0\n",
    "    train_epocs(model1, optimizer1, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
