{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from spacy.symbols import ORTH\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file into DataFrame\n",
    "df = pd.read_csv('./CAMEO_IDEA_labeled_data.csv')\n",
    "\n",
    "# separate content and label\n",
    "text = df['Content']\n",
    "labels = df['Category Code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion tokenize sentence\n",
    "tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = tokenizer.Defaults.stop_words\n",
    "# tokenize, lemmatize the text, drop punctuations and stopwords\n",
    "tokenize = lambda t: [token.lemma_ for token in tokenizer(t) if (not token.is_punct) and (not token.is_stop)]\n",
    "\n",
    "# only tokenize the text\n",
    "# tokenize = lambda t: [token.text for token in tokenizer(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary <key=word : value=count>\n",
    "cnt = Counter()\n",
    "size = text.size\n",
    "for idx in range(size):\n",
    "    for word in tokenize(text[idx]):\n",
    "        cnt[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = dict(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out low-frequency word\n",
    "min_threshold = 1\n",
    "count = {x: count for x, count in cnt.items() if count >= min_threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out high-frequency word\n",
    "max_threshold = 1\n",
    "count = {x: count for x, count in cnt.items() if count <= max_threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(text)\n",
    "y = np.array(labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download glove dictionary\n",
    "# def download_glove():\n",
    "#     ! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#     ! unzip glove.6B.zip -C data\n",
    "    \n",
    "# download_glove()\n",
    "# ! unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embedding dictionary (<key=word : value=vector>)\n",
    "def load_embedding_dict():\n",
    "    embeddings_dict = {}\n",
    "    with open(\"glove.6B.50d.txt\", 'r') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "glove_dic = load_embedding_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries(<key=word : value=index number>) (<key=word : value=vector>)\n",
    "def create_embedding_matrix(count,emb_size=50):\n",
    "    size = len(count) + 2\n",
    "    word_idx_dict = {}\n",
    "    word_vec = np.zeros((size, emb_size), dtype=\"float32\")\n",
    "    \n",
    "    # add padding and UNK keyword\n",
    "    word_idx_dict[\"\"] = 0\n",
    "    word_vec[0] = np.zeros(emb_size, dtype='float32')\n",
    "    word_idx_dict[\"UNK\"] = 1\n",
    "    word_vec[1] = np.random.uniform(-0.25, 0.25, emb_size)\n",
    "\n",
    "    for i, word in enumerate(count.keys()):\n",
    "        word_idx_dict[word] = i + 2\n",
    "\n",
    "        if word in glove_dic:\n",
    "            word_vec[i + 2] = glove_dic[word]\n",
    "        else:\n",
    "            word_vec[i + 2] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "\n",
    "    return word_idx_dict, word_vec\n",
    "    \n",
    "word_idx_dict, pretrained_weight = create_embedding_matrix(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for encoding sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(line, word_idx_dict, N=400, padding_start=True):\n",
    "    tokens = tokenize(line)\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([word_idx_dict.get(word, word_idx_dict[\"UNK\"]) for word in tokens])\n",
    "    length = min(N, len(enc1))\n",
    "    if padding_start:\n",
    "        enc[:length] = enc1[:length]\n",
    "    else:\n",
    "        enc[N - length:] = enc1[:length]\n",
    "    return enc, length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DataSet and DataLoader for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventDataset(Dataset):\n",
    "    def __init__(self, X, y, N=40, padding_start=False):\n",
    "        self.y = y\n",
    "        self.X = [encode_sentence(line, word_idx_dict, N, padding_start) for line in X]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, s = self.X[idx]\n",
    "        return x, s, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = EventDataset(X_train, y_train)\n",
    "valid_ds = EventDataset(X_val, y_val)\n",
    "train_dl = DataLoader(train_ds, batch_size=30, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_optimizer(optimizer, lr):\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epocs(model, optimizer, train_dl, val_dl, epochs=10):\n",
    "    global max_acc\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, s, y in train_dl:\n",
    "            # s is not used in this model\n",
    "            x = x.long() \n",
    "            y = y.float() \n",
    "#             x = x.long()\n",
    "#             y = y.float()\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = F.binary_cross_entropy_with_logits(y_pred, y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc = val_metrics(model, val_dl)\n",
    "        if val_acc > max_acc:\n",
    "            max_acc = val_acc\n",
    "        if i % 5 == 0:\n",
    "            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))\n",
    "#             print(y.numpy())\n",
    "            \n",
    "#             print()\n",
    "#             print(confusion_matrix(y.numpy(), F.softmax(y_pred).detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for x, s, y in valid_dl:\n",
    "        # s is not used here\n",
    "        x = x.long()\n",
    "        y = y.float().unsqueeze(1)\n",
    "#         x = x.long()\n",
    "#         y = y.float().unsqueeze(1)\n",
    "        y_hat = model(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        y_pred = y_hat > 0\n",
    "        correct += (y_pred.float() == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights=None) :\n",
    "        super(GRUModel,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if glove_weights is not None:\n",
    "            self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "            self.embeddings.weight.requires_grad = True ## freeze embeddings\n",
    "            \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "#         print(x.size())\n",
    "        out_pack, ht = self.gru(x)\n",
    "        x = self.linear(ht[-1])\n",
    "#         print(x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.357 val loss 0.298 and val accuracy 0.899\n",
      "train loss 0.069 val loss 0.275 and val accuracy 0.899\n",
      "train loss 0.046 val loss 0.362 and val accuracy 0.892\n",
      "train loss 0.039 val loss 0.358 and val accuracy 0.886\n",
      "train loss 0.032 val loss 0.367 and val accuracy 0.899\n",
      "train loss 0.037 val loss 0.420 and val accuracy 0.892\n",
      "tensor(0.9051)\n",
      "train loss 0.376 val loss 0.246 and val accuracy 0.911\n",
      "train loss 0.082 val loss 0.562 and val accuracy 0.892\n",
      "train loss 0.054 val loss 0.444 and val accuracy 0.892\n",
      "train loss 0.091 val loss 0.560 and val accuracy 0.905\n",
      "train loss 0.076 val loss 0.340 and val accuracy 0.886\n",
      "train loss 0.074 val loss 0.607 and val accuracy 0.899\n",
      "tensor(0.9114)\n",
      "train loss 0.319 val loss 0.273 and val accuracy 0.911\n",
      "train loss 0.086 val loss 0.277 and val accuracy 0.899\n",
      "train loss 0.068 val loss 0.316 and val accuracy 0.899\n",
      "train loss 0.040 val loss 0.318 and val accuracy 0.918\n",
      "train loss 0.037 val loss 0.324 and val accuracy 0.905\n",
      "train loss 0.030 val loss 0.347 and val accuracy 0.905\n",
      "tensor(0.9177)\n",
      "train loss 0.809 val loss 0.481 and val accuracy 0.899\n",
      "train loss 0.203 val loss 0.352 and val accuracy 0.899\n",
      "train loss 0.114 val loss 0.321 and val accuracy 0.918\n",
      "train loss 0.082 val loss 0.367 and val accuracy 0.911\n",
      "train loss 0.068 val loss 0.503 and val accuracy 0.911\n",
      "train loss 0.066 val loss 0.425 and val accuracy 0.905\n",
      "tensor(0.9177)\n",
      "train loss 0.346 val loss 0.264 and val accuracy 0.905\n",
      "train loss 0.076 val loss 0.373 and val accuracy 0.899\n",
      "train loss 0.034 val loss 0.352 and val accuracy 0.905\n",
      "train loss 0.046 val loss 0.306 and val accuracy 0.911\n",
      "train loss 0.035 val loss 0.376 and val accuracy 0.918\n",
      "train loss 0.033 val loss 0.406 and val accuracy 0.899\n",
      "tensor(0.9241)\n",
      "train loss 0.485 val loss 0.258 and val accuracy 0.905\n",
      "train loss 0.423 val loss 0.620 and val accuracy 0.905\n",
      "train loss 0.304 val loss 0.602 and val accuracy 0.918\n",
      "train loss 0.544 val loss 1.330 and val accuracy 0.905\n",
      "train loss 0.289 val loss 0.571 and val accuracy 0.905\n",
      "train loss 0.257 val loss 0.470 and val accuracy 0.899\n",
      "tensor(0.9241)\n",
      "train loss 0.353 val loss 0.314 and val accuracy 0.899\n",
      "train loss 0.089 val loss 0.256 and val accuracy 0.924\n",
      "train loss 0.050 val loss 0.306 and val accuracy 0.918\n",
      "train loss 0.048 val loss 0.264 and val accuracy 0.905\n",
      "train loss 0.041 val loss 0.367 and val accuracy 0.911\n",
      "train loss 0.033 val loss 0.321 and val accuracy 0.918\n",
      "tensor(0.9241)\n",
      "train loss 0.800 val loss 0.425 and val accuracy 0.899\n",
      "train loss 0.254 val loss 0.373 and val accuracy 0.867\n",
      "train loss 0.137 val loss 0.379 and val accuracy 0.892\n",
      "train loss 0.255 val loss 0.950 and val accuracy 0.899\n",
      "train loss 0.394 val loss 0.574 and val accuracy 0.880\n",
      "train loss 0.257 val loss 0.834 and val accuracy 0.892\n",
      "tensor(0.9114)\n"
     ]
    }
   ],
   "source": [
    "for i in range(4): \n",
    "    vocab_size = len(word_idx_dict)\n",
    "    model = GRUModel(vocab_size, 50, 50*(i+1), glove_weights = pretrained_weight)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.01)\n",
    "    max_acc = 0\n",
    "    train_epocs(model, optimizer, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)\n",
    "    \n",
    "    vocab_size = len(word_idx_dict)\n",
    "    model = GRUModel(vocab_size, 50, 50*(i+1), glove_weights = pretrained_weight)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.1)\n",
    "    max_acc = 0\n",
    "    train_epocs(model, optimizer, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel1(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, hidden_dim1, glove_weights=None) :\n",
    "        super(GRUModel1,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if glove_weights is not None:\n",
    "            self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "            self.embeddings.weight.requires_grad = True ## freeze embeddings\n",
    "            \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim1)\n",
    "        self.linear1 = nn.Linear(hidden_dim1, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "#         print(x.size())\n",
    "        out_pack, ht = self.gru(x)\n",
    "        x = self.linear(ht[-1])\n",
    "        x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.351 val loss 0.273 and val accuracy 0.905\n",
      "train loss 0.068 val loss 0.472 and val accuracy 0.918\n",
      "train loss 0.042 val loss 0.281 and val accuracy 0.924\n",
      "train loss 0.033 val loss 0.331 and val accuracy 0.918\n",
      "train loss 0.030 val loss 0.416 and val accuracy 0.918\n",
      "train loss 0.039 val loss 0.419 and val accuracy 0.880\n",
      "tensor(0.9304)\n",
      "train loss 1.662 val loss 0.459 and val accuracy 0.899\n",
      "train loss 0.240 val loss 0.352 and val accuracy 0.892\n",
      "train loss 0.076 val loss 0.434 and val accuracy 0.886\n",
      "train loss 0.074 val loss 1.062 and val accuracy 0.481\n",
      "train loss 0.087 val loss 0.434 and val accuracy 0.873\n",
      "train loss 0.036 val loss 0.586 and val accuracy 0.886\n",
      "tensor(0.9114)\n",
      "train loss 0.430 val loss 0.282 and val accuracy 0.905\n",
      "train loss 0.090 val loss 0.267 and val accuracy 0.918\n",
      "train loss 0.048 val loss 0.247 and val accuracy 0.924\n",
      "train loss 0.030 val loss 0.357 and val accuracy 0.924\n",
      "train loss 0.032 val loss 0.368 and val accuracy 0.924\n",
      "train loss 0.032 val loss 0.366 and val accuracy 0.911\n",
      "tensor(0.9304)\n",
      "train loss 2.604 val loss 2.936 and val accuracy 0.899\n",
      "train loss 0.129 val loss 0.445 and val accuracy 0.899\n",
      "train loss 0.084 val loss 0.342 and val accuracy 0.854\n",
      "train loss 0.065 val loss 0.380 and val accuracy 0.867\n",
      "train loss 0.076 val loss 0.390 and val accuracy 0.892\n",
      "train loss 0.076 val loss 0.308 and val accuracy 0.905\n",
      "tensor(0.9051)\n",
      "train loss 0.403 val loss 0.280 and val accuracy 0.905\n",
      "train loss 0.091 val loss 0.282 and val accuracy 0.911\n",
      "train loss 0.049 val loss 0.306 and val accuracy 0.918\n",
      "train loss 0.031 val loss 0.372 and val accuracy 0.911\n",
      "train loss 0.034 val loss 0.296 and val accuracy 0.924\n",
      "train loss 0.039 val loss 0.415 and val accuracy 0.918\n",
      "tensor(0.9304)\n",
      "train loss 5.602 val loss 2.089 and val accuracy 0.892\n",
      "train loss 0.324 val loss 0.321 and val accuracy 0.892\n",
      "train loss 0.334 val loss 0.362 and val accuracy 0.886\n",
      "train loss 40.625 val loss 5.699 and val accuracy 0.886\n",
      "train loss 0.685 val loss 1.458 and val accuracy 0.892\n",
      "train loss 0.190 val loss 0.318 and val accuracy 0.899\n",
      "tensor(0.8987)\n",
      "train loss 0.353 val loss 0.284 and val accuracy 0.899\n",
      "train loss 0.086 val loss 0.253 and val accuracy 0.905\n",
      "train loss 0.055 val loss 0.308 and val accuracy 0.911\n",
      "train loss 0.046 val loss 0.304 and val accuracy 0.918\n",
      "train loss 0.041 val loss 0.421 and val accuracy 0.899\n",
      "train loss 0.029 val loss 0.360 and val accuracy 0.899\n",
      "tensor(0.9304)\n",
      "train loss 4.871 val loss 11.180 and val accuracy 0.127\n",
      "train loss 0.143 val loss 0.360 and val accuracy 0.892\n",
      "train loss 0.066 val loss 0.404 and val accuracy 0.886\n",
      "train loss 0.055 val loss 0.363 and val accuracy 0.848\n",
      "train loss 0.045 val loss 0.311 and val accuracy 0.886\n",
      "train loss 0.047 val loss 0.285 and val accuracy 0.873\n",
      "tensor(0.9114)\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    model1 = GRUModel1(vocab_size, 50, 50*(i+1), 50, glove_weights = pretrained_weight)\n",
    "    parameters1 = filter(lambda p: p.requires_grad, model1.parameters())\n",
    "    optimizer1 = torch.optim.Adam(parameters1, lr=0.01)\n",
    "    max_acc = 0\n",
    "    train_epocs(model1, optimizer1, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)\n",
    "    \n",
    "    model1 = GRUModel1(vocab_size, 50, 50*(i+1), 50, glove_weights = pretrained_weight)\n",
    "    parameters1 = filter(lambda p: p.requires_grad, model1.parameters())\n",
    "    optimizer1 = torch.optim.Adam(parameters1, lr=0.1)\n",
    "    max_acc = 0\n",
    "    train_epocs(model1, optimizer1, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
