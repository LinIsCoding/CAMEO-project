{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from spacy.symbols import ORTH\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file into DataFrame\n",
    "df = pd.read_csv('./CAMEO_IDEA_labeled_data.csv')\n",
    "\n",
    "# separate content and label\n",
    "text = df['Content']\n",
    "labels = df['Category Code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion tokenize sentence\n",
    "tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = tokenizer.Defaults.stop_words\n",
    "# tokenize, lemmatize the text, drop punctuations and stopwords\n",
    "tokenize = lambda t: [token.lemma_ for token in tokenizer(t) if (not token.is_punct) and (not token.is_stop)]\n",
    "\n",
    "# only tokenize the text\n",
    "# tokenize = lambda t: [token.text for token in tokenizer(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary <key=word : value=count>\n",
    "cnt = Counter()\n",
    "size = text.size\n",
    "for idx in range(size):\n",
    "    for word in tokenize(text[idx]):\n",
    "        cnt[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out low-frequency word\n",
    "min_threshold = 1\n",
    "count = {x: count for x, count in cnt.items() if count >= min_threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out high-frequency word\n",
    "min_threshold = 1\n",
    "count = {x: count for x, count in cnt.items() if count <= min_threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(text)\n",
    "y = np.array(labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download glove dictionary\n",
    "# def download_glove():\n",
    "#     ! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#     ! unzip glove.6B.zip -C data\n",
    "    \n",
    "# download_glove()\n",
    "# ! unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embedding dictionary (<key=word : value=vector>)\n",
    "def load_embedding_dict():\n",
    "    embeddings_dict = {}\n",
    "    with open(\"glove.6B.50d.txt\", 'r') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "glove_dic = load_embedding_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries(<key=word : value=index number>) (<key=word : value=vector>)\n",
    "def create_embedding_matrix(count,emb_size=50):\n",
    "    size = len(count) + 2\n",
    "    word_idx_dict = {}\n",
    "    word_vec = np.zeros((size, emb_size), dtype=\"float32\")\n",
    "    \n",
    "    # add padding and UNK keyword\n",
    "    word_idx_dict[\"\"] = 0\n",
    "    word_vec[0] = np.zeros(emb_size, dtype='float32')\n",
    "    word_idx_dict[\"UNK\"] = 1\n",
    "    word_vec[1] = np.random.uniform(-0.25, 0.25, emb_size)\n",
    "\n",
    "    for i, word in enumerate(count.keys()):\n",
    "        word_idx_dict[word] = i + 2\n",
    "\n",
    "        if word in glove_dic:\n",
    "            word_vec[i + 2] = glove_dic[word]\n",
    "        else:\n",
    "            word_vec[i + 2] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "\n",
    "    return word_idx_dict, word_vec\n",
    "    \n",
    "word_idx_dict, pretrained_weight = create_embedding_matrix(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for encoding sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(line, word_idx_dict, N=400, padding_start=True):\n",
    "    tokens = tokenize(line)\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([word_idx_dict.get(word, word_idx_dict[\"UNK\"]) for word in tokens])\n",
    "    length = min(N, len(enc1))\n",
    "    if padding_start:\n",
    "        enc[:length] = enc1[:length]\n",
    "    else:\n",
    "        enc[N - length:] = enc1[:length]\n",
    "    return enc, length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DataSet and DataLoader for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventDataset(Dataset):\n",
    "    def __init__(self, X, y, N=40, padding_start=False):\n",
    "        self.y = y\n",
    "        self.X = [encode_sentence(line, word_idx_dict, N, padding_start) for line in X]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, s = self.X[idx]\n",
    "        return x, s, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = EventDataset(X_train, y_train)\n",
    "valid_ds = EventDataset(X_val, y_val)\n",
    "train_dl = DataLoader(train_ds, batch_size=30, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_optimizer(optimizer, lr):\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epocs(model, optimizer, train_dl, val_dl, epochs=10):\n",
    "    global max_acc\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, s, y in train_dl:\n",
    "            # s is not used in this model\n",
    "            x = x.long() \n",
    "            y = y.long() \n",
    "#             x = x.long()\n",
    "#             y = y.float()\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "#             print(y_pred.size())\n",
    "#             print(y.size())\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc = val_metrics(model, val_dl)\n",
    "        if val_acc > max_acc:\n",
    "            max_acc = val_acc\n",
    "        if i % 5 == 0:\n",
    "            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))\n",
    "#             print(y.numpy())\n",
    "            \n",
    "#             print()\n",
    "#             print(confusion_matrix(y.numpy(), F.softmax(y_pred).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, val_dl):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    for x, s, y in val_dl:\n",
    "        x = x.long()  #.cuda()\n",
    "        y = y.long()\n",
    "        batch = y.shape[0]\n",
    "#         print(y.size())\n",
    "        out = model(x)\n",
    "        \n",
    "        loss = F.cross_entropy(out, y)\n",
    "        sum_loss += batch*(loss.item())\n",
    "        total += batch\n",
    "        _, pred = torch.max(out, 1) \n",
    "#         print(pred.size())\n",
    "        correct += (pred == y.data).float().sum().item()\n",
    "    val_loss = sum_loss/total\n",
    "    val_acc = correct/total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1]], dtype=torch.int32),\n",
       " tensor([15, 15, 16, 23, 18, 17, 16, 19, 11, 19, 15, 16,  8, 18, 15,  6, 16, 16,\n",
       "         11, 19, 14, 12, 23, 19, 22, 16, 21, 16, 15, 12]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(valid_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights=None) :\n",
    "        super(GRUModel,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if glove_weights is not None:\n",
    "            self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "            self.embeddings.weight.requires_grad = True ## freeze embeddings\n",
    "            \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "#         print(x.size())\n",
    "        out_pack, ht = self.gru(x)\n",
    "        x = self.linear(ht[-1])\n",
    "#         print(x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4): \n",
    "    vocab_size = len(word_idx_dict)\n",
    "    model = GRUModel(vocab_size, 50, 50*(i+1), glove_weights = pretrained_weight)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.01)\n",
    "    max_acc = 0\n",
    "    train_epocs(model, optimizer, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)\n",
    "    \n",
    "    vocab_size = len(word_idx_dict)\n",
    "    model = GRUModel(vocab_size, 50, 50*(i+1), glove_weights = pretrained_weight)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=0.1)\n",
    "    max_acc = 0\n",
    "    train_epocs(model, optimizer, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel1(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, hidden_dim1, glove_weights=None) :\n",
    "        super(GRUModel1,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if glove_weights is not None:\n",
    "            self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "            self.embeddings.weight.requires_grad = True ## freeze embeddings\n",
    "            \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim1)\n",
    "        self.linear1 = nn.Linear(hidden_dim1, 3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "#         print(x.size())\n",
    "        out_pack, ht = self.gru(x)\n",
    "        x = self.linear(ht[-1])\n",
    "        x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.503 val loss 0.342 and val accuracy 0.899\n",
      "train loss 0.077 val loss 0.339 and val accuracy 0.911\n",
      "train loss 0.034 val loss 0.437 and val accuracy 0.905\n",
      "train loss 0.043 val loss 0.281 and val accuracy 0.911\n",
      "train loss 0.041 val loss 0.281 and val accuracy 0.899\n",
      "train loss 0.032 val loss 0.314 and val accuracy 0.905\n",
      "0.9240506329113924\n",
      "train loss 2.279 val loss 1.073 and val accuracy 0.899\n",
      "train loss 0.293 val loss 0.506 and val accuracy 0.899\n",
      "train loss 0.168 val loss 0.303 and val accuracy 0.911\n",
      "train loss 0.578 val loss 2.433 and val accuracy 0.899\n",
      "train loss 13.683 val loss 22.516 and val accuracy 0.892\n",
      "train loss 1.547 val loss 5.832 and val accuracy 0.905\n",
      "0.9113924050632911\n",
      "train loss 0.413 val loss 0.321 and val accuracy 0.899\n",
      "train loss 0.064 val loss 0.331 and val accuracy 0.924\n",
      "train loss 0.050 val loss 0.288 and val accuracy 0.918\n",
      "train loss 0.042 val loss 0.281 and val accuracy 0.924\n",
      "train loss 0.035 val loss 0.332 and val accuracy 0.918\n",
      "train loss 0.033 val loss 0.426 and val accuracy 0.899\n",
      "0.930379746835443\n",
      "train loss 4.319 val loss 4.174 and val accuracy 0.899\n",
      "train loss 1.025 val loss 0.815 and val accuracy 0.886\n",
      "train loss 0.317 val loss 0.845 and val accuracy 0.911\n",
      "train loss 5.039 val loss 27.562 and val accuracy 0.133\n",
      "train loss 9.788 val loss 11.220 and val accuracy 0.899\n",
      "train loss 5.127 val loss 10.030 and val accuracy 0.918\n",
      "0.9177215189873418\n",
      "train loss 0.493 val loss 0.296 and val accuracy 0.899\n",
      "train loss 0.092 val loss 0.278 and val accuracy 0.905\n",
      "train loss 0.051 val loss 0.243 and val accuracy 0.924\n",
      "train loss 0.037 val loss 0.489 and val accuracy 0.911\n",
      "train loss 0.034 val loss 0.330 and val accuracy 0.930\n",
      "train loss 0.036 val loss 0.310 and val accuracy 0.911\n",
      "0.930379746835443\n",
      "train loss 15.421 val loss 8.811 and val accuracy 0.899\n",
      "train loss 0.218 val loss 0.663 and val accuracy 0.911\n",
      "train loss 0.219 val loss 1.780 and val accuracy 0.899\n",
      "train loss 0.800 val loss 4.491 and val accuracy 0.899\n",
      "train loss 9.675 val loss 12.053 and val accuracy 0.899\n",
      "train loss 2.454 val loss 3.597 and val accuracy 0.892\n",
      "0.9240506329113924\n",
      "train loss 0.386 val loss 0.275 and val accuracy 0.899\n",
      "train loss 0.083 val loss 0.394 and val accuracy 0.918\n",
      "train loss 0.042 val loss 0.364 and val accuracy 0.905\n",
      "train loss 0.041 val loss 0.360 and val accuracy 0.905\n",
      "train loss 0.039 val loss 0.343 and val accuracy 0.911\n",
      "train loss 0.027 val loss 0.370 and val accuracy 0.918\n",
      "0.930379746835443\n",
      "train loss 16.777 val loss 7.596 and val accuracy 0.899\n",
      "train loss 12.403 val loss 17.193 and val accuracy 0.101\n",
      "train loss 0.307 val loss 0.405 and val accuracy 0.899\n",
      "train loss 0.175 val loss 0.457 and val accuracy 0.892\n",
      "train loss 0.110 val loss 0.411 and val accuracy 0.899\n",
      "train loss 0.059 val loss 0.455 and val accuracy 0.905\n",
      "0.9113924050632911\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    model1 = GRUModel1(vocab_size, 50, 50*(i+1), 50, glove_weights = pretrained_weight)\n",
    "    parameters1 = filter(lambda p: p.requires_grad, model1.parameters())\n",
    "    optimizer1 = torch.optim.Adam(parameters1, lr=0.01)\n",
    "    max_acc = 0\n",
    "    train_epocs(model1, optimizer1, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)\n",
    "    \n",
    "    model1 = GRUModel1(vocab_size, 50, 50*(i+1), 50, glove_weights = pretrained_weight)\n",
    "    parameters1 = filter(lambda p: p.requires_grad, model1.parameters())\n",
    "    optimizer1 = torch.optim.Adam(parameters1, lr=0.1)\n",
    "    max_acc = 0\n",
    "    train_epocs(model1, optimizer1, train_dl, valid_dl, epochs=30)\n",
    "    print(max_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
